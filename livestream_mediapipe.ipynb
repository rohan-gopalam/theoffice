{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello world\n"
     ]
    }
   ],
   "source": [
    "print(\"hello world\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: requests in /Users/xuanzhang/Library/Python/3.9/lib/python/site-packages (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/xuanzhang/Library/Python/3.9/lib/python/site-packages (from requests) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/xuanzhang/Library/Python/3.9/lib/python/site-packages (from requests) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/xuanzhang/Library/Python/3.9/lib/python/site-packages (from requests) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/xuanzhang/Library/Python/3.9/lib/python/site-packages (from requests) (2024.2.2)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded to detector.tflite\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "url = \"https://storage.googleapis.com/mediapipe-models/face_detector/blaze_face_short_range/float16/1/blaze_face_short_range.tflite\"\n",
    "output_path = \"detector.tflite\"\n",
    "\n",
    "response = requests.get(url)\n",
    "if response.status_code == 200:\n",
    "    with open(output_path, \"wb\") as f:\n",
    "        f.write(response.content)\n",
    "    print(f\"Downloaded to {output_path}\")\n",
    "else:\n",
    "    print(f\"Failed to download. HTTP Status Code: {response.status_code}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: mediapipe in /Users/xuanzhang/Library/Python/3.9/lib/python/site-packages (0.10.21)\n",
      "Requirement already satisfied: absl-py in /Users/xuanzhang/Library/Python/3.9/lib/python/site-packages (from mediapipe) (2.1.0)\n",
      "Requirement already satisfied: attrs>=19.1.0 in /Users/xuanzhang/Library/Python/3.9/lib/python/site-packages (from mediapipe) (23.2.0)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in /Users/xuanzhang/Library/Python/3.9/lib/python/site-packages (from mediapipe) (25.2.10)\n",
      "Requirement already satisfied: jax in /Users/xuanzhang/Library/Python/3.9/lib/python/site-packages (from mediapipe) (0.4.30)\n",
      "Requirement already satisfied: jaxlib in /Users/xuanzhang/Library/Python/3.9/lib/python/site-packages (from mediapipe) (0.4.30)\n",
      "Requirement already satisfied: matplotlib in /Users/xuanzhang/Library/Python/3.9/lib/python/site-packages (from mediapipe) (3.9.4)\n",
      "Requirement already satisfied: numpy<2 in /Users/xuanzhang/Library/Python/3.9/lib/python/site-packages (from mediapipe) (1.26.4)\n",
      "Requirement already satisfied: opencv-contrib-python in /Users/xuanzhang/Library/Python/3.9/lib/python/site-packages (from mediapipe) (4.11.0.86)\n",
      "Requirement already satisfied: protobuf<5,>=4.25.3 in /Users/xuanzhang/Library/Python/3.9/lib/python/site-packages (from mediapipe) (4.25.3)\n",
      "Requirement already satisfied: sounddevice>=0.4.4 in /Users/xuanzhang/Library/Python/3.9/lib/python/site-packages (from mediapipe) (0.5.1)\n",
      "Requirement already satisfied: sentencepiece in /Users/xuanzhang/Library/Python/3.9/lib/python/site-packages (from mediapipe) (0.2.0)\n",
      "Requirement already satisfied: CFFI>=1.0 in /Users/xuanzhang/Library/Python/3.9/lib/python/site-packages (from sounddevice>=0.4.4->mediapipe) (1.17.1)\n",
      "Requirement already satisfied: ml-dtypes>=0.2.0 in /Users/xuanzhang/Library/Python/3.9/lib/python/site-packages (from jax->mediapipe) (0.4.1)\n",
      "Requirement already satisfied: opt-einsum in /Users/xuanzhang/Library/Python/3.9/lib/python/site-packages (from jax->mediapipe) (3.4.0)\n",
      "Requirement already satisfied: scipy>=1.9 in /Users/xuanzhang/Library/Python/3.9/lib/python/site-packages (from jax->mediapipe) (1.13.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.6 in /Users/xuanzhang/Library/Python/3.9/lib/python/site-packages (from jax->mediapipe) (8.6.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/xuanzhang/Library/Python/3.9/lib/python/site-packages (from matplotlib->mediapipe) (1.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/xuanzhang/Library/Python/3.9/lib/python/site-packages (from matplotlib->mediapipe) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/xuanzhang/Library/Python/3.9/lib/python/site-packages (from matplotlib->mediapipe) (4.56.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /Users/xuanzhang/Library/Python/3.9/lib/python/site-packages (from matplotlib->mediapipe) (1.4.7)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/xuanzhang/Library/Python/3.9/lib/python/site-packages (from matplotlib->mediapipe) (23.2)\n",
      "Requirement already satisfied: pillow>=8 in /Users/xuanzhang/Library/Python/3.9/lib/python/site-packages (from matplotlib->mediapipe) (10.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Users/xuanzhang/Library/Python/3.9/lib/python/site-packages (from matplotlib->mediapipe) (3.2.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/xuanzhang/Library/Python/3.9/lib/python/site-packages (from matplotlib->mediapipe) (2.8.2)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in /Users/xuanzhang/Library/Python/3.9/lib/python/site-packages (from matplotlib->mediapipe) (6.5.2)\n",
      "Requirement already satisfied: pycparser in /Users/xuanzhang/Library/Python/3.9/lib/python/site-packages (from CFFI>=1.0->sounddevice>=0.4.4->mediapipe) (2.22)\n",
      "Requirement already satisfied: zipp>=3.20 in /Users/xuanzhang/Library/Python/3.9/lib/python/site-packages (from importlib-metadata>=4.6->jax->mediapipe) (3.21.0)\n",
      "Requirement already satisfied: six>=1.5 in /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/site-packages (from python-dateutil>=2.7->matplotlib->mediapipe) (1.15.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install mediapipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'audio_classifier' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmediapipe\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mmp\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmediapipe\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtasks\u001b[39;00m \u001b[39mimport\u001b[39;00m python\n\u001b[1;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmediapipe\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtasks\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m \u001b[39mimport\u001b[39;00m vision\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/mediapipe/__init__.py:17\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmediapipe\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[1;32m     16\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmediapipe\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msolutions\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39msolutions\u001b[39;00m \n\u001b[0;32m---> 17\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmediapipe\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtasks\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mtasks\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[39mdel\u001b[39;00m framework\n\u001b[1;32m     21\u001b[0m \u001b[39mdel\u001b[39;00m gpu\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/mediapipe/tasks/python/__init__.py:17\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Copyright 2022 The MediaPipe Authors.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39m#\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[39m# See the License for the specific language governing permissions and\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[39m# limitations under the License.\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[39m\"\"\"MediaPipe Tasks API.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m audio\n\u001b[1;32m     18\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m components\n\u001b[1;32m     19\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m core\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/mediapipe/tasks/python/audio/__init__.py:21\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmediapipe\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtasks\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39maudio\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39maudio_classifier\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmediapipe\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtasks\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39maudio\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39maudio_embedder\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m AudioClassifier \u001b[39m=\u001b[39m audio_classifier\u001b[39m.\u001b[39mAudioClassifier\n\u001b[1;32m     22\u001b[0m AudioClassifierOptions \u001b[39m=\u001b[39m audio_classifier\u001b[39m.\u001b[39mAudioClassifierOptions\n\u001b[1;32m     23\u001b[0m AudioClassifierResult \u001b[39m=\u001b[39m audio_classifier\u001b[39m.\u001b[39mAudioClassifierResult\n",
      "\u001b[0;31mNameError\u001b[0m: name 'audio_classifier' is not defined"
     ]
    }
   ],
   "source": [
    "import mediapipe as mp\n",
    "from mediapipe.tasks import python\n",
    "from mediapipe.tasks.python import vision\n",
    "import cv2\n",
    "\n",
    "detection_results = []  # Global variable to store results for annotation\n",
    "def print_result(result, output_image, timestamp_ms):\n",
    "    global detection_results\n",
    "    detection_results = result.detections  # Store detections for annotation\n",
    "\n",
    "# Visualization function\n",
    "def visualize(image, detections):\n",
    "    \"\"\"Draw bounding boxes on the image.\"\"\"\n",
    "    annotated_image = image.copy()\n",
    "    height, width, _ = image.shape\n",
    "    for detection in detections:\n",
    "        bbox = detection.bounding_box\n",
    "        start_point = (bbox.origin_x, bbox.origin_y)\n",
    "        end_point = (bbox.origin_x + bbox.width, bbox.origin_y + bbox.height)\n",
    "        # Draw the bounding box\n",
    "        cv2.rectangle(annotated_image, start_point, end_point, (0, 255, 0), 2)\n",
    "    return annotated_image\n",
    "\n",
    "\n",
    "\n",
    "# Load the MediaPipe model\n",
    "BaseOptions = mp.tasks.BaseOptions\n",
    "FaceDetector = mp.tasks.vision.FaceDetector\n",
    "FaceDetectorOptions = mp.tasks.vision.FaceDetectorOptions\n",
    "VisionRunningMode = mp.tasks.vision.RunningMode\n",
    "\n",
    "# Path to the downloaded model\n",
    "model_path = \"detector.tflite\"\n",
    "\n",
    "# Set options for live stream\n",
    "options = FaceDetectorOptions(\n",
    "    base_options=BaseOptions(model_asset_path=model_path),\n",
    "    running_mode=VisionRunningMode.LIVE_STREAM,\n",
    "    result_callback=print_result\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from fer import FER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1736291397.193483  228414 gl_context.cc:369] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M4\n",
      "I0000 00:00:1736291397.199847  228414 gl_context.cc:369] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M4\n",
      "W0000 00:00:1736291397.205173  229901 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1736291397.212812  229901 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1736291397.265896  229910 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1736291397.273748  229914 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Camera is on. Press 'q' to quit.\n",
      "Turning off the camera.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "# from deepface import DeepFace\n",
    "\n",
    "# Initialize MediaPipe Face Mesh\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "face_mesh = mp_face_mesh.FaceMesh(static_image_mode=False, max_num_faces=1, min_detection_confidence=0.5)\n",
    "\n",
    "\n",
    "# Initialize MediaPipe Pose\n",
    "mp_pose = mp.solutions.pose\n",
    "pose = mp_pose.Pose()\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "\n",
    "\n",
    "# Initialize FER\n",
    "#emotion_detector = FER(mtcnn=True)\n",
    "\n",
    "# Access the webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Unable to access the camera.\")\n",
    "else:\n",
    "    print(\"Camera is on. Press 'q' to quit.\")\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            print(\"Error: Unable to read frame.\")\n",
    "            break\n",
    "\n",
    "        # Convert the frame to RGB for MediaPipe and FER\n",
    "        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # Detect face mesh\n",
    "        results_face = face_mesh.process(rgb_frame)\n",
    "        if results_face.multi_face_landmarks:\n",
    "            for face_landmarks in results_face.multi_face_landmarks:\n",
    "                # Draw the facial landmarks on the frame\n",
    "                mp.solutions.drawing_utils.draw_landmarks(\n",
    "                    frame, face_landmarks, mp_face_mesh.FACEMESH_TESSELATION,\n",
    "                    mp.solutions.drawing_styles.get_default_face_mesh_tesselation_style())\n",
    "\n",
    "\n",
    "        # try:\n",
    "        #     result = DeepFace.analyze(frame, actions=['emotion'], enforce_detection=False)\n",
    "        #     # Display dominant emotion on the frame\n",
    "        #     dominant_emotion = max(result['emotion'], key=result['emotion'].get)\n",
    "        #     cv2.putText(frame, dominant_emotion, (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "        # except Exception as e:\n",
    "        #     print(\"No face detected:\", str(e))\n",
    "    \n",
    "\n",
    "        # if results_face.multi_face_landmarks:\n",
    "        #     for face_landmarks in results_face.multi_face_landmarks:\n",
    "        #         # Example: Detect smile\n",
    "        #         landmarks = face_landmarks.landmark\n",
    "        #         # Calculate distances\n",
    "        #         left_corner = landmarks[61]  # Left corner of mouth\n",
    "        #         right_corner = landmarks[291]  # Right corner of mouth\n",
    "        #         upper_lip = landmarks[13]  # Upper lip\n",
    "        #         lower_lip = landmarks[14]  # Lower lip\n",
    "\n",
    "        #         # Horizontal distance (mouth width)\n",
    "        #         mouth_width = ((right_corner.x - left_corner.x) ** 2 +\n",
    "        #                     (right_corner.y - left_corner.y) ** 2) ** 0.5\n",
    "        #         # Vertical distance (mouth openness)\n",
    "        #         mouth_height = ((upper_lip.x - lower_lip.x) ** 2 +\n",
    "        #                         (upper_lip.y - lower_lip.y) ** 2) ** 0.5\n",
    "\n",
    "        #         # Define a smile condition\n",
    "        #         if mouth_width / mouth_height > 3:  # Adjust the threshold as needed\n",
    "        #             cv2.putText(frame, \"Smiling\", (50, 50),\n",
    "        #                         cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "\n",
    "        # Process the frame to detect pose\n",
    "        results_pose = pose.process(rgb_frame)\n",
    "\n",
    "        # Draw landmarks on the frame\n",
    "        if results_pose.pose_landmarks:\n",
    "            mp_drawing.draw_landmarks(\n",
    "                frame, results_pose.pose_landmarks, mp_pose.POSE_CONNECTIONS\n",
    "            )\n",
    "\n",
    "        # Display the frame\n",
    "        cv2.imshow(\"MediaPipe Pose\", frame)\n",
    "\n",
    "\n",
    "\n",
    "        # Detect emotions\n",
    "        # emotions = emotion_detector.detect_emotions(frame)\n",
    "        # if emotions:\n",
    "        #     dominant_emotion, emotion_probabilities = emotions[0]['emotions'].items(), emotions[0]['emotions']\n",
    "        #     emotion_text = f\"Emotion: {max(emotion_probabilities, key=emotion_probabilities.get)}\"\n",
    "        #     cv2.putText(frame, emotion_text, (10, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)\n",
    "\n",
    "        # Display the frame\n",
    "        cv2.imshow('Face Mesh & Emotion Detection', frame)\n",
    "\n",
    "        # Exit when 'q' is pressed\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            print(\"Turning off the camera.\")\n",
    "            break\n",
    "\n",
    "# Release resources\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1733909744.847078 2698198 gl_context.cc:357] GL version: 2.1 (2.1 ATI-4.14.1), renderer: AMD Radeon Pro 555 OpenGL Engine\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "W0000 00:00:1733909744.874495 2698901 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "#old code just keeping to reference\n",
    "\n",
    "import cv2\n",
    "\n",
    "\n",
    "\n",
    "# Access the webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Unable to access the camera.\")\n",
    "    exit()\n",
    "\n",
    "# Initialize a manual timestamp counter\n",
    "current_timestamp_ms = 0\n",
    "timestamp_increment = 33  # Adjust for your camera's frame rate\n",
    "\n",
    "# Create the face detector instance\n",
    "with FaceDetector.create_from_options(options) as detector:\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            print(\"Failed to capture frame.\")\n",
    "            break\n",
    "\n",
    "        # Resize frame to improve performance\n",
    "        frame = cv2.resize(frame, (640, 480))\n",
    "\n",
    "        # Convert frame to RGB\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # Convert to MediaPipe Image\n",
    "        mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=frame_rgb)\n",
    "\n",
    "        try:\n",
    "            # Increment and send manual timestamp\n",
    "            current_timestamp_ms += timestamp_increment\n",
    "            detector.detect_async(mp_image, current_timestamp_ms)\n",
    "        except ValueError as e:\n",
    "            print(f\"Error in detection: {e}\")\n",
    "            break\n",
    "\n",
    "\n",
    "        # Draw bounding boxes on the frame using the latest results\n",
    "        annotated_frame = visualize(frame, detection_results)\n",
    "\n",
    "        # Display the annotated frame\n",
    "        cv2.imshow(\"Face Detection\", annotated_frame)\n",
    "        \n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):  # Exit on 'q' key press\n",
    "            break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
