{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello world\n"
     ]
    }
   ],
   "source": [
    "print(\"hello world\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting requests\n",
      "  Using cached requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting charset-normalizer<4,>=2 (from requests)\n",
      "  Downloading charset_normalizer-3.4.1-cp312-cp312-macosx_10_13_universal2.whl.metadata (35 kB)\n",
      "Collecting idna<4,>=2.5 (from requests)\n",
      "  Using cached idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests)\n",
      "  Using cached urllib3-2.3.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests)\n",
      "  Using cached certifi-2024.12.14-py3-none-any.whl.metadata (2.3 kB)\n",
      "Using cached requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "Using cached certifi-2024.12.14-py3-none-any.whl (164 kB)\n",
      "Downloading charset_normalizer-3.4.1-cp312-cp312-macosx_10_13_universal2.whl (196 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.1/196.1 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached idna-3.10-py3-none-any.whl (70 kB)\n",
      "Using cached urllib3-2.3.0-py3-none-any.whl (128 kB)\n",
      "Installing collected packages: urllib3, idna, charset-normalizer, certifi, requests\n",
      "Successfully installed certifi-2024.12.14 charset-normalizer-3.4.1 idna-3.10 requests-2.32.3 urllib3-2.3.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3 install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded to detector.tflite\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "url = \"https://storage.googleapis.com/mediapipe-models/face_detector/blaze_face_short_range/float16/1/blaze_face_short_range.tflite\"\n",
    "output_path = \"detector.tflite\"\n",
    "\n",
    "response = requests.get(url)\n",
    "if response.status_code == 200:\n",
    "    with open(output_path, \"wb\") as f:\n",
    "        f.write(response.content)\n",
    "    print(f\"Downloaded to {output_path}\")\n",
    "else:\n",
    "    print(f\"Failed to download. HTTP Status Code: {response.status_code}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting mediapipe\n",
      "  Downloading mediapipe-0.10.20-cp312-cp312-macosx_11_0_universal2.whl.metadata (9.7 kB)\n",
      "Collecting absl-py (from mediapipe)\n",
      "  Using cached absl_py-2.1.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting attrs>=19.1.0 (from mediapipe)\n",
      "  Using cached attrs-24.3.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting flatbuffers>=2.0 (from mediapipe)\n",
      "  Using cached flatbuffers-24.12.23-py2.py3-none-any.whl.metadata (876 bytes)\n",
      "Collecting jax (from mediapipe)\n",
      "  Downloading jax-0.4.38-py3-none-any.whl.metadata (22 kB)\n",
      "Collecting jaxlib (from mediapipe)\n",
      "  Downloading jaxlib-0.4.38-cp312-cp312-macosx_11_0_arm64.whl.metadata (1.0 kB)\n",
      "Collecting matplotlib (from mediapipe)\n",
      "  Downloading matplotlib-3.10.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (11 kB)\n",
      "Collecting numpy<2 (from mediapipe)\n",
      "  Downloading numpy-1.26.4-cp312-cp312-macosx_11_0_arm64.whl.metadata (61 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.1/61.1 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting opencv-contrib-python (from mediapipe)\n",
      "  Using cached opencv_contrib_python-4.10.0.84-cp37-abi3-macosx_11_0_arm64.whl.metadata (20 kB)\n",
      "Collecting protobuf<5,>=4.25.3 (from mediapipe)\n",
      "  Using cached protobuf-4.25.5-cp37-abi3-macosx_10_9_universal2.whl.metadata (541 bytes)\n",
      "Collecting sounddevice>=0.4.4 (from mediapipe)\n",
      "  Using cached sounddevice-0.5.1-py3-none-macosx_10_6_x86_64.macosx_10_6_universal2.whl.metadata (1.4 kB)\n",
      "Collecting sentencepiece (from mediapipe)\n",
      "  Downloading sentencepiece-0.2.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (7.7 kB)\n",
      "Collecting CFFI>=1.0 (from sounddevice>=0.4.4->mediapipe)\n",
      "  Downloading cffi-1.17.1-cp312-cp312-macosx_11_0_arm64.whl.metadata (1.5 kB)\n",
      "Collecting ml_dtypes>=0.4.0 (from jax->mediapipe)\n",
      "  Downloading ml_dtypes-0.5.1-cp312-cp312-macosx_10_9_universal2.whl.metadata (21 kB)\n",
      "Collecting opt_einsum (from jax->mediapipe)\n",
      "  Using cached opt_einsum-3.4.0-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting scipy>=1.10 (from jax->mediapipe)\n",
      "  Downloading scipy-1.15.0-cp312-cp312-macosx_14_0_arm64.whl.metadata (61 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting contourpy>=1.0.1 (from matplotlib->mediapipe)\n",
      "  Downloading contourpy-1.3.1-cp312-cp312-macosx_11_0_arm64.whl.metadata (5.4 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib->mediapipe)\n",
      "  Using cached cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib->mediapipe)\n",
      "  Downloading fonttools-4.55.3-cp312-cp312-macosx_10_13_universal2.whl.metadata (165 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m165.1/165.1 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting kiwisolver>=1.3.1 (from matplotlib->mediapipe)\n",
      "  Downloading kiwisolver-1.4.8-cp312-cp312-macosx_11_0_arm64.whl.metadata (6.2 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/neelkolhe/Library/Python/3.12/lib/python/site-packages (from matplotlib->mediapipe) (24.2)\n",
      "Collecting pillow>=8 (from matplotlib->mediapipe)\n",
      "  Downloading pillow-11.1.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (9.1 kB)\n",
      "Collecting pyparsing>=2.3.1 (from matplotlib->mediapipe)\n",
      "  Using cached pyparsing-3.2.1-py3-none-any.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/neelkolhe/Library/Python/3.12/lib/python/site-packages (from matplotlib->mediapipe) (2.9.0.post0)\n",
      "Collecting pycparser (from CFFI>=1.0->sounddevice>=0.4.4->mediapipe)\n",
      "  Using cached pycparser-2.22-py3-none-any.whl.metadata (943 bytes)\n",
      "Requirement already satisfied: six>=1.5 in /Users/neelkolhe/Library/Python/3.12/lib/python/site-packages (from python-dateutil>=2.7->matplotlib->mediapipe) (1.17.0)\n",
      "Downloading mediapipe-0.10.20-cp312-cp312-macosx_11_0_universal2.whl (49.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.2/49.2 MB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached attrs-24.3.0-py3-none-any.whl (63 kB)\n",
      "Using cached flatbuffers-24.12.23-py2.py3-none-any.whl (30 kB)\n",
      "Downloading numpy-1.26.4-cp312-cp312-macosx_11_0_arm64.whl (13.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.7/13.7 MB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hUsing cached protobuf-4.25.5-cp37-abi3-macosx_10_9_universal2.whl (394 kB)\n",
      "Using cached sounddevice-0.5.1-py3-none-macosx_10_6_x86_64.macosx_10_6_universal2.whl (107 kB)\n",
      "Using cached absl_py-2.1.0-py3-none-any.whl (133 kB)\n",
      "Downloading jax-0.4.38-py3-none-any.whl (2.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0mm\n",
      "\u001b[?25hDownloading jaxlib-0.4.38-cp312-cp312-macosx_11_0_arm64.whl (79.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 MB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading matplotlib-3.10.0-cp312-cp312-macosx_11_0_arm64.whl (8.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.0/8.0 MB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached opencv_contrib_python-4.10.0.84-cp37-abi3-macosx_11_0_arm64.whl (63.7 MB)\n",
      "Downloading sentencepiece-0.2.0-cp312-cp312-macosx_11_0_arm64.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading cffi-1.17.1-cp312-cp312-macosx_11_0_arm64.whl (178 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m178.8/178.8 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading contourpy-1.3.1-cp312-cp312-macosx_11_0_arm64.whl (255 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m255.4/255.4 kB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Downloading fonttools-4.55.3-cp312-cp312-macosx_10_13_universal2.whl (2.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading kiwisolver-1.4.8-cp312-cp312-macosx_11_0_arm64.whl (65 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.1/65.1 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading ml_dtypes-0.5.1-cp312-cp312-macosx_10_9_universal2.whl (670 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m670.4/670.4 kB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading pillow-11.1.0-cp312-cp312-macosx_11_0_arm64.whl (3.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached pyparsing-3.2.1-py3-none-any.whl (107 kB)\n",
      "Downloading scipy-1.15.0-cp312-cp312-macosx_14_0_arm64.whl (24.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.9/24.9 MB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached opt_einsum-3.4.0-py3-none-any.whl (71 kB)\n",
      "Using cached pycparser-2.22-py3-none-any.whl (117 kB)\n",
      "Installing collected packages: sentencepiece, flatbuffers, pyparsing, pycparser, protobuf, pillow, opt_einsum, numpy, kiwisolver, fonttools, cycler, attrs, absl-py, scipy, opencv-contrib-python, ml_dtypes, contourpy, CFFI, sounddevice, matplotlib, jaxlib, jax, mediapipe\n",
      "Successfully installed CFFI-1.17.1 absl-py-2.1.0 attrs-24.3.0 contourpy-1.3.1 cycler-0.12.1 flatbuffers-24.12.23 fonttools-4.55.3 jax-0.4.38 jaxlib-0.4.38 kiwisolver-1.4.8 matplotlib-3.10.0 mediapipe-0.10.20 ml_dtypes-0.5.1 numpy-1.26.4 opencv-contrib-python-4.10.0.84 opt_einsum-3.4.0 pillow-11.1.0 protobuf-4.25.5 pycparser-2.22 pyparsing-3.2.1 scipy-1.15.0 sentencepiece-0.2.0 sounddevice-0.5.1\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3 install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install mediapipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mediapipe as mp\n",
    "from mediapipe.tasks import python\n",
    "from mediapipe.tasks.python import vision\n",
    "import cv2\n",
    "\n",
    "detection_results = []  # Global variable to store results for annotation\n",
    "def print_result(result, output_image, timestamp_ms):\n",
    "    global detection_results\n",
    "    detection_results = result.detections  # Store detections for annotation\n",
    "\n",
    "# Visualization function\n",
    "def visualize(image, detections):\n",
    "    \"\"\"Draw bounding boxes on the image.\"\"\"\n",
    "    annotated_image = image.copy()\n",
    "    height, width, _ = image.shape\n",
    "    for detection in detections:\n",
    "        bbox = detection.bounding_box\n",
    "        start_point = (bbox.origin_x, bbox.origin_y)\n",
    "        end_point = (bbox.origin_x + bbox.width, bbox.origin_y + bbox.height)\n",
    "        # Draw the bounding box\n",
    "        cv2.rectangle(annotated_image, start_point, end_point, (0, 255, 0), 2)\n",
    "    return annotated_image\n",
    "\n",
    "\n",
    "\n",
    "# Load the MediaPipe model\n",
    "BaseOptions = mp.tasks.BaseOptions\n",
    "FaceDetector = mp.tasks.vision.FaceDetector\n",
    "FaceDetectorOptions = mp.tasks.vision.FaceDetectorOptions\n",
    "VisionRunningMode = mp.tasks.vision.RunningMode\n",
    "\n",
    "# Path to the downloaded model\n",
    "model_path = \"detector.tflite\"\n",
    "\n",
    "# Set options for live stream\n",
    "options = FaceDetectorOptions(\n",
    "    base_options=BaseOptions(model_asset_path=model_path),\n",
    "    running_mode=VisionRunningMode.LIVE_STREAM,\n",
    "    result_callback=print_result\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from fer import FER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1736291397.193483  228414 gl_context.cc:369] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M4\n",
      "I0000 00:00:1736291397.199847  228414 gl_context.cc:369] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M4\n",
      "W0000 00:00:1736291397.205173  229901 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1736291397.212812  229901 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1736291397.265896  229910 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1736291397.273748  229914 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Camera is on. Press 'q' to quit.\n",
      "Turning off the camera.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "# from deepface import DeepFace\n",
    "\n",
    "# Initialize MediaPipe Face Mesh\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "face_mesh = mp_face_mesh.FaceMesh(static_image_mode=False, max_num_faces=1, min_detection_confidence=0.5)\n",
    "\n",
    "\n",
    "# Initialize MediaPipe Pose\n",
    "mp_pose = mp.solutions.pose\n",
    "pose = mp_pose.Pose()\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "\n",
    "\n",
    "# Initialize FER\n",
    "#emotion_detector = FER(mtcnn=True)\n",
    "\n",
    "# Access the webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Unable to access the camera.\")\n",
    "else:\n",
    "    print(\"Camera is on. Press 'q' to quit.\")\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            print(\"Error: Unable to read frame.\")\n",
    "            break\n",
    "\n",
    "        # Convert the frame to RGB for MediaPipe and FER\n",
    "        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # Detect face mesh\n",
    "        results_face = face_mesh.process(rgb_frame)\n",
    "        if results_face.multi_face_landmarks:\n",
    "            for face_landmarks in results_face.multi_face_landmarks:\n",
    "                # Draw the facial landmarks on the frame\n",
    "                mp.solutions.drawing_utils.draw_landmarks(\n",
    "                    frame, face_landmarks, mp_face_mesh.FACEMESH_TESSELATION,\n",
    "                    mp.solutions.drawing_styles.get_default_face_mesh_tesselation_style())\n",
    "\n",
    "\n",
    "        # try:\n",
    "        #     result = DeepFace.analyze(frame, actions=['emotion'], enforce_detection=False)\n",
    "        #     # Display dominant emotion on the frame\n",
    "        #     dominant_emotion = max(result['emotion'], key=result['emotion'].get)\n",
    "        #     cv2.putText(frame, dominant_emotion, (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "        # except Exception as e:\n",
    "        #     print(\"No face detected:\", str(e))\n",
    "    \n",
    "\n",
    "        # if results_face.multi_face_landmarks:\n",
    "        #     for face_landmarks in results_face.multi_face_landmarks:\n",
    "        #         # Example: Detect smile\n",
    "        #         landmarks = face_landmarks.landmark\n",
    "        #         # Calculate distances\n",
    "        #         left_corner = landmarks[61]  # Left corner of mouth\n",
    "        #         right_corner = landmarks[291]  # Right corner of mouth\n",
    "        #         upper_lip = landmarks[13]  # Upper lip\n",
    "        #         lower_lip = landmarks[14]  # Lower lip\n",
    "\n",
    "        #         # Horizontal distance (mouth width)\n",
    "        #         mouth_width = ((right_corner.x - left_corner.x) ** 2 +\n",
    "        #                     (right_corner.y - left_corner.y) ** 2) ** 0.5\n",
    "        #         # Vertical distance (mouth openness)\n",
    "        #         mouth_height = ((upper_lip.x - lower_lip.x) ** 2 +\n",
    "        #                         (upper_lip.y - lower_lip.y) ** 2) ** 0.5\n",
    "\n",
    "        #         # Define a smile condition\n",
    "        #         if mouth_width / mouth_height > 3:  # Adjust the threshold as needed\n",
    "        #             cv2.putText(frame, \"Smiling\", (50, 50),\n",
    "        #                         cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "\n",
    "        # Process the frame to detect pose\n",
    "        results_pose = pose.process(rgb_frame)\n",
    "\n",
    "        # Draw landmarks on the frame\n",
    "        if results_pose.pose_landmarks:\n",
    "            mp_drawing.draw_landmarks(\n",
    "                frame, results_pose.pose_landmarks, mp_pose.POSE_CONNECTIONS\n",
    "            )\n",
    "\n",
    "        # Display the frame\n",
    "        cv2.imshow(\"MediaPipe Pose\", frame)\n",
    "\n",
    "\n",
    "\n",
    "        # Detect emotions\n",
    "        # emotions = emotion_detector.detect_emotions(frame)\n",
    "        # if emotions:\n",
    "        #     dominant_emotion, emotion_probabilities = emotions[0]['emotions'].items(), emotions[0]['emotions']\n",
    "        #     emotion_text = f\"Emotion: {max(emotion_probabilities, key=emotion_probabilities.get)}\"\n",
    "        #     cv2.putText(frame, emotion_text, (10, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)\n",
    "\n",
    "        # Display the frame\n",
    "        cv2.imshow('Face Mesh & Emotion Detection', frame)\n",
    "\n",
    "        # Exit when 'q' is pressed\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            print(\"Turning off the camera.\")\n",
    "            break\n",
    "\n",
    "# Release resources\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1733909744.847078 2698198 gl_context.cc:357] GL version: 2.1 (2.1 ATI-4.14.1), renderer: AMD Radeon Pro 555 OpenGL Engine\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "W0000 00:00:1733909744.874495 2698901 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "#old code just keeping to reference\n",
    "\n",
    "import cv2\n",
    "\n",
    "\n",
    "\n",
    "# Access the webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Unable to access the camera.\")\n",
    "    exit()\n",
    "\n",
    "# Initialize a manual timestamp counter\n",
    "current_timestamp_ms = 0\n",
    "timestamp_increment = 33  # Adjust for your camera's frame rate\n",
    "\n",
    "# Create the face detector instance\n",
    "with FaceDetector.create_from_options(options) as detector:\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            print(\"Failed to capture frame.\")\n",
    "            break\n",
    "\n",
    "        # Resize frame to improve performance\n",
    "        frame = cv2.resize(frame, (640, 480))\n",
    "\n",
    "        # Convert frame to RGB\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # Convert to MediaPipe Image\n",
    "        mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=frame_rgb)\n",
    "\n",
    "        try:\n",
    "            # Increment and send manual timestamp\n",
    "            current_timestamp_ms += timestamp_increment\n",
    "            detector.detect_async(mp_image, current_timestamp_ms)\n",
    "        except ValueError as e:\n",
    "            print(f\"Error in detection: {e}\")\n",
    "            break\n",
    "\n",
    "\n",
    "        # Draw bounding boxes on the frame using the latest results\n",
    "        annotated_frame = visualize(frame, detection_results)\n",
    "\n",
    "        # Display the annotated frame\n",
    "        cv2.imshow(\"Face Detection\", annotated_frame)\n",
    "        \n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):  # Exit on 'q' key press\n",
    "            break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "office",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
