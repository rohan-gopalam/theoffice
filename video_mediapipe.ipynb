{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello world\n"
     ]
    }
   ],
   "source": [
    "print(\"hello world\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded to detector.tflite\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "url = \"https://storage.googleapis.com/mediapipe-models/face_detector/blaze_face_short_range/float16/1/blaze_face_short_range.tflite\"\n",
    "output_path = \"detector.tflite\"\n",
    "\n",
    "response = requests.get(url)\n",
    "if response.status_code == 200:\n",
    "    with open(output_path, \"wb\") as f:\n",
    "        f.write(response.content)\n",
    "    print(f\"Downloaded to {output_path}\")\n",
    "else:\n",
    "    print(f\"Failed to download. HTTP Status Code: {response.status_code}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-19 00:12:37.362141: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1734595966.406759 3505242 gl_context.cc:357] GL version: 2.1 (2.1 ATI-4.14.1), renderer: AMD Radeon Pro 555 OpenGL Engine\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "I0000 00:00:1734595966.453648 3505242 gl_context.cc:357] GL version: 2.1 (2.1 ATI-4.14.1), renderer: AMD Radeon Pro 555 OpenGL Engine\n",
      "W0000 00:00:1734595966.454684 3505652 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1734595967.200851 3505820 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1734595967.263641 3505814 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video URL: https://rr1---sn-jxopj-n5oe.googlevideo.com/videoplayback?expire=1734617569&ei=gdVjZ_DrJ5yRsfIPtaWqkAg&ip=2607%3Af140%3A400%3A6b%3A8481%3Aaa15%3A119c%3Ac39f&id=o-AO3pEOPxVTVyCv8oqKzlidrx0sNkVg8gIm9Mi6wuQRHm&itag=18&source=youtube&requiressl=yes&xpc=EgVo2aDSNQ%3D%3D&met=1734595969%2C&mh=KA&mm=31%2C29&mn=sn-jxopj-n5oe%2Csn-n4v7sns7&ms=au%2Crdu&mv=m&mvi=1&pl=32&rms=au%2Cau&initcwndbps=4940000&bui=AfMhrI_b9ex8kXD4fFoKml691V91_dxEtkTjjDc4O6OB-2phtLZDLOyvhE6iz0yH2trVC0gB6w44t4yM&vprv=1&svpuc=1&mime=video%2Fmp4&ns=qwniYdHrbdCs9SMZRmlO3z8Q&rqh=1&gir=yes&clen=124173805&ratebypass=yes&dur=2031.931&lmt=1733188644547912&mt=1734595518&fvip=5&fexp=51326932%2C51331020%2C51335594%2C51353498%2C51371294&c=MWEB&sefc=1&txp=4438434&n=kEau8q3La-Xn-Q&sparams=expire%2Cei%2Cip%2Cid%2Citag%2Csource%2Crequiressl%2Cxpc%2Cbui%2Cvprv%2Csvpuc%2Cmime%2Cns%2Crqh%2Cgir%2Cclen%2Cratebypass%2Cdur%2Clmt&sig=AJfQdSswRAIgZk1kYXkEZdFiygNps4PBNRZjMk_MB1YSkn8MgtYKfA0CIFu1HPa0M06hf3YCycSlnMp45T2c4RAlHaws2t2O140F&lsparams=met%2Cmh%2Cmm%2Cmn%2Cms%2Cmv%2Cmvi%2Cpl%2Crms%2Cinitcwndbps&lsig=AGluJ3MwRQIhAP8r4dNG0-sSoUV7l0LEzkI2GLQCXiNvWjxTqdQhrDU9AiBsdAZZoLvRKf8D7FtI5ljAia6innBYIXrHljS757tlvg%3D%3D\n",
      "Processing YouTube video. Press 'q' to quit.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1734595973.660287 3505813 landmark_projection_calculator.cc:186] Using NORM_RECT without IMAGE_DIMENSIONS is only supported for the square ROI. Provide IMAGE_DIMENSIONS or use PROJECTION_MATRIX.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frame 40: Assigned Person ID 1\n",
      "Frame 45: Assigned Person ID 2\n",
      "Frame 65: Assigned Person ID 3\n",
      "Frame 85: Assigned Person ID 2\n",
      "Frame 90: Assigned Person ID 3\n",
      "Frame 95: Assigned Person ID 3\n",
      "Frame 100: Assigned Person ID 3\n",
      "Frame 100: Assigned Person ID 3\n",
      "Frame 105: Assigned Person ID 3\n",
      "Frame 105: Assigned Person ID 3\n",
      "Frame 110: Assigned Person ID 3\n",
      "Frame 110: Assigned Person ID 3\n",
      "Frame 115: Assigned Person ID 3\n",
      "Frame 120: Assigned Person ID 3\n",
      "Frame 125: Assigned Person ID 2\n",
      "Frame 130: Assigned Person ID 3\n",
      "Frame 135: Assigned Person ID 3\n",
      "Frame 140: Assigned Person ID 3\n",
      "Frame 185: Assigned Person ID 1\n",
      "Frame 225: Assigned Person ID 1\n",
      "Frame 230: Assigned Person ID 2\n",
      "Frame 235: Assigned Person ID 3\n",
      "Frame 240: Assigned Person ID 4\n",
      "Frame 245: Assigned Person ID 5\n",
      "Frame 250: Assigned Person ID 6\n",
      "Frame 255: Assigned Person ID 7\n",
      "Exiting video processing.\n",
      "Profiles saved to profiles.json\n",
      "Processed 255 frames in 21.69 seconds.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import time\n",
    "\n",
    "# Function to fetch the direct video URL using yt-dlp\n",
    "def get_video_url(youtube_url):\n",
    "    ydl_opts = {\n",
    "        'quiet': True,\n",
    "        'format': 'best[ext=mp4]'\n",
    "    }\n",
    "    with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
    "        info = ydl.extract_info(youtube_url, download=False)\n",
    "        return info['url']\n",
    "\n",
    "# Initialize MediaPipe solutions\n",
    "mp_face_detection = mp.solutions.face_detection\n",
    "mp_pose = mp.solutions.pose\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# Initialize Face Detection and Pose Detection\n",
    "face_detection = mp_face_detection.FaceDetection(min_detection_confidence=0.9)\n",
    "pose = mp_pose.Pose(min_detection_confidence=0.8)\n",
    "\n",
    "profiles = {}\n",
    "\n",
    "# Function to assign or update ID based on face embedding and spatial proximity\n",
    "def assign_id(embedding, bbox, profiles, threshold=0.6, spatial_threshold=50):\n",
    "    best_match_id = None\n",
    "    best_similarity = float(\"inf\")\n",
    "    best_distance = float(\"inf\")\n",
    "\n",
    "    bbox_center = ((bbox[0] + bbox[2] / 2), (bbox[1] + bbox[3] / 2))\n",
    "\n",
    "    for profile_id, profile in profiles.items():\n",
    "        # Compare embedding similarity\n",
    "        similarity = np.linalg.norm(np.array(profile['face_embedding']) - np.array(embedding))\n",
    "        # Compare spatial proximity\n",
    "        profile_bbox = profile[\"bbox\"]\n",
    "        profile_center = ((profile_bbox[0] + profile_bbox[2] / 2), (profile_bbox[1] + profile_bbox[3] / 2))\n",
    "        spatial_distance = euclidean(profile_center, bbox_center)\n",
    "\n",
    "        # Find the best match\n",
    "        if similarity < threshold and spatial_distance < spatial_threshold:\n",
    "            if similarity < best_similarity or (similarity == best_similarity and spatial_distance < best_distance):\n",
    "                best_match_id = profile_id\n",
    "                best_similarity = similarity\n",
    "                best_distance = spatial_distance\n",
    "\n",
    "    if best_match_id is not None:\n",
    "        # Update the matched profile\n",
    "        profiles[best_match_id][\"bbox\"] = bbox\n",
    "        profiles[best_match_id][\"last_seen\"] = frame_count\n",
    "        return best_match_id\n",
    "    else:\n",
    "        # Create a new profile\n",
    "        new_id = len(profiles) + 1\n",
    "        profiles[new_id] = {\"face_embedding\": embedding, \"bbox\": bbox, \"last_seen\": frame_count}\n",
    "        return new_id\n",
    "\n",
    "\n",
    "# YouTube video URL\n",
    "youtube_url = \"https://www.youtube.com/watch?v=9D0WmzB5ovY\"  # Replace with your video URL\n",
    "\n",
    "# Process a YouTube video\n",
    "youtube_url = \"https://www.youtube.com/watch?v=96Y6mc3C1Bg\"  # Replace with your URL\n",
    "video_url = get_video_url(youtube_url)\n",
    "print(\"Video URL:\", video_url)\n",
    "\n",
    "cap = cv2.VideoCapture(video_url)\n",
    "\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Unable to open the YouTube video stream.\")\n",
    "else:\n",
    "    print(\"Processing YouTube video. Press 'q' to quit.\")\n",
    "\n",
    "\n",
    "    # Specify the start time (in seconds)\n",
    "    start_time = 320  # Start the video at 30 seconds\n",
    "    cap.set(cv2.CAP_PROP_POS_MSEC, start_time * 1000)  # Jump to the start time\n",
    "\n",
    "\n",
    "    # Adjustable frame rate (delay in milliseconds)\n",
    "    frame_delay_ms = 100  # Adjust this value to control the frame rate\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            print(\"End of video or error reading the video stream.\")\n",
    "            break\n",
    "\n",
    "        # Skip frames for efficiency\n",
    "        if frame_count % 5 != 0:\n",
    "            frame_count += 1\n",
    "            continue\n",
    "\n",
    "        # Validate frame dimensions\n",
    "        if frame is None or frame.shape[0] == 0 or frame.shape[1] == 0:\n",
    "            print(f\"Skipped frame {frame_count}: invalid dimensions.\")\n",
    "            continue\n",
    "\n",
    "        # Get frame dimensions\n",
    "        ih, iw, _ = frame.shape\n",
    "\n",
    "        # Convert frame to RGB for MediaPipe\n",
    "        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Face Detection\n",
    "        face_results = face_detection.process(rgb_frame)\n",
    "        face_bboxes = []\n",
    "\n",
    "        if face_results.detections:\n",
    "            for i, detection in enumerate(face_results.detections):\n",
    "                # Extract bounding box\n",
    "                bboxC = detection.location_data.relative_bounding_box\n",
    "                ih, iw, _ = frame.shape\n",
    "                x, y, w, h = (int(bboxC.xmin * iw), int(bboxC.ymin * ih),\n",
    "                              int(bboxC.width * iw), int(bboxC.height * ih))\n",
    "                face_bboxes.append((x, y, w, h))\n",
    "\n",
    "                # Draw bounding box around the face\n",
    "                cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "\n",
    "                # Add label for each face\n",
    "                cv2.putText(frame, f\"Face {i+1}\", (x, y - 10),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)\n",
    "\n",
    "        # Pose Detection\n",
    "        pose_results = pose.process(rgb_frame)\n",
    "        if pose_results.pose_landmarks:\n",
    "            filtered_landmarks = []\n",
    "            for i, lm in enumerate(pose_results.pose_landmarks.landmark):\n",
    "                x, y = int(lm.x * iw), int(lm.y * ih)\n",
    "                inside_face = any(fx <= x <= fx + fw and fy <= y <= fy + fh for fx, fy, fw, fh in face_bboxes)\n",
    "                if not inside_face:  # Only include landmarks outside face bounding boxes\n",
    "                    filtered_landmarks.append((x, y))\n",
    "\n",
    "            # Draw filtered pose landmarks\n",
    "            for x, y in filtered_landmarks:\n",
    "                cv2.circle(frame, (x, y), 3, (0, 0, 255), -1)\n",
    "\n",
    "\n",
    "        # Display the processed frame\n",
    "        cv2.imshow(\"MediaPipe YouTube Video Processing\", frame)\n",
    "\n",
    "        # Adjustable frame delay\n",
    "        time.sleep(frame_delay_ms / 1000.0)  # Convert ms to seconds\n",
    "\n",
    "        # Exit when 'q' is pressed\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            print(\"Exiting video processing.\")\n",
    "            break\n",
    "\n",
    "# Release resources\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1734596265.413802 3508182 gl_context.cc:357] GL version: 2.1 (2.1 ATI-4.14.1), renderer: AMD Radeon Pro 555 OpenGL Engine\n",
      "W0000 00:00:1734596265.419068 3511817 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video URL: https://rr1---sn-jxopj-n5oe.googlevideo.com/videoplayback?expire=1734617868&ei=rNZjZ4y3A4yWsfIPyr3w2QI&ip=2607%3Af140%3A400%3A6b%3A8481%3Aaa15%3A119c%3Ac39f&id=o-AFtpSAoHUUj2X_nEuvUDEBJTYDYOW6ZtWTWpFkaX3dDc&itag=18&source=youtube&requiressl=yes&xpc=EgVo2aDSNQ%3D%3D&met=1734596268%2C&mh=KA&mm=31%2C29&mn=sn-jxopj-n5oe%2Csn-o097znse&ms=au%2Crdu&mv=m&mvi=1&pl=32&rms=au%2Cau&initcwndbps=4312500&bui=AfMhrI8OH-KxSNZ0tDqUW_7UVK_xXBKcBpaWy6FGqrSoAmBVQXmcIerS02jQxVQNL2Q-4bORx6Ok_5wH&vprv=1&svpuc=1&mime=video%2Fmp4&ns=0wKDSEkWV9J-LPo8ZdaX_2MQ&rqh=1&gir=yes&clen=124173805&ratebypass=yes&dur=2031.931&lmt=1733188644547912&mt=1734596001&fvip=2&fexp=51326932%2C51331020%2C51335594%2C51371293&c=MWEB&sefc=1&txp=4438434&n=O3wOY7x5bLucgQ&sparams=expire%2Cei%2Cip%2Cid%2Citag%2Csource%2Crequiressl%2Cxpc%2Cbui%2Cvprv%2Csvpuc%2Cmime%2Cns%2Crqh%2Cgir%2Cclen%2Cratebypass%2Cdur%2Clmt&sig=AJfQdSswRQIgdx9x9lNA9o5W4EyaISxwiVh1r2BYkEp2wNZSk7CiY3kCIQCdv9yLG42oqt9bTNgYNJgQX5iQVrNd1NTC0H-bni661Q%3D%3D&lsparams=met%2Cmh%2Cmm%2Cmn%2Cms%2Cmv%2Cmvi%2Cpl%2Crms%2Cinitcwndbps&lsig=AGluJ3MwRAIgGT0K4NyiF0fSga6Th7IBs_x57JjvAkaqYnH830uLvqICID_hvyDBJFhXSrAuugpsmgQ7vyVIz8Jinu7B4l7RT4Ba\n",
      "Processing YouTube video. Press 'q' to quit.\n",
      "Exiting video processing.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'json' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 147\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;66;03m# Save profiles to JSON\u001b[39;00m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprofiles.json\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m--> 147\u001b[0m     \u001b[43mjson\u001b[49m\u001b[38;5;241m.\u001b[39mdump(profiles, f, indent\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m)\n\u001b[1;32m    148\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProfiles saved to profiles.json\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'json' is not defined"
     ]
    }
   ],
   "source": [
    "import yt_dlp\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import time\n",
    "import numpy as np\n",
    "from deepface import DeepFace\n",
    "from scipy.spatial.distance import euclidean\n",
    "\n",
    "# Function to fetch the direct video URL using yt-dlp\n",
    "def get_video_url(youtube_url):\n",
    "    ydl_opts = {\n",
    "        'quiet': True,\n",
    "        'format': 'best[ext=mp4]'\n",
    "    }\n",
    "    with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
    "        info = ydl.extract_info(youtube_url, download=False)\n",
    "        return info['url']\n",
    "\n",
    "# Initialize MediaPipe solutions\n",
    "mp_face_detection = mp.solutions.face_detection\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# Initialize Face Detection\n",
    "face_detection = mp_face_detection.FaceDetection(min_detection_confidence=0.5)\n",
    "\n",
    "# Profiles for tracking people\n",
    "profiles = {}\n",
    "max_inactive_frames = 30  # Maximum frames to keep a profile if not detected\n",
    "\n",
    "# Function to assign or update ID based on face embedding and spatial proximity\n",
    "def assign_id(embedding, bbox, profiles, frame_count, threshold=0.6, spatial_threshold=50):\n",
    "    best_match_id = None\n",
    "    best_similarity = float(\"inf\")\n",
    "    best_distance = float(\"inf\")\n",
    "\n",
    "    bbox_center = ((bbox[0] + bbox[2] / 2), (bbox[1] + bbox[3] / 2))\n",
    "\n",
    "    for profile_id, profile in profiles.items():\n",
    "        # Compare embedding similarity\n",
    "        similarity = np.linalg.norm(np.array(profile['face_embedding']) - np.array(embedding))\n",
    "        # Compare spatial proximity\n",
    "        profile_bbox = profile[\"bbox\"]\n",
    "        profile_center = ((profile_bbox[0] + profile_bbox[2] / 2), (profile_bbox[1] + profile_bbox[3] / 2))\n",
    "        spatial_distance = euclidean(profile_center, bbox_center)\n",
    "\n",
    "        # Find the best match\n",
    "        if similarity < threshold and spatial_distance < spatial_threshold:\n",
    "            if similarity < best_similarity or (similarity == best_similarity and spatial_distance < best_distance):\n",
    "                best_match_id = profile_id\n",
    "                best_similarity = similarity\n",
    "                best_distance = spatial_distance\n",
    "\n",
    "    if best_match_id is not None:\n",
    "        # Update the matched profile\n",
    "        profiles[best_match_id][\"bbox\"] = bbox\n",
    "        profiles[best_match_id][\"last_seen\"] = frame_count\n",
    "        return best_match_id\n",
    "    else:\n",
    "        # Create a new profile\n",
    "        new_id = len(profiles) + 1\n",
    "        profiles[new_id] = {\"face_embedding\": embedding, \"bbox\": bbox, \"last_seen\": frame_count}\n",
    "        return new_id\n",
    "\n",
    "# YouTube video URL\n",
    "youtube_url = \"https://www.youtube.com/watch?v=9D0WmzB5ovY\"  # Replace with your video URL\n",
    "\n",
    "# Fetch the video stream URL\n",
    "video_url = get_video_url(youtube_url)\n",
    "print(\"Video URL:\", video_url)\n",
    "\n",
    "# Open the video stream with OpenCV\n",
    "cap = cv2.VideoCapture(video_url)\n",
    "\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Unable to open the YouTube video stream.\")\n",
    "else:\n",
    "    print(\"Processing YouTube video. Press 'q' to quit.\")\n",
    "\n",
    "    # Specify the start time (in seconds)\n",
    "    start_time = 30  # Start the video at 30 seconds\n",
    "    cap.set(cv2.CAP_PROP_POS_MSEC, start_time * 1000)  # Jump to the start time\n",
    "\n",
    "    # Adjustable frame rate (delay in milliseconds)\n",
    "    frame_delay_ms = 100  # Adjust this value to control the frame rate\n",
    "    frame_count = 0  # Initialize frame counter\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            print(\"End of video or error reading the video stream.\")\n",
    "            break\n",
    "\n",
    "        # Increment frame count\n",
    "        frame_count += 1\n",
    "\n",
    "        # Convert the frame to RGB for MediaPipe\n",
    "        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # Detect faces in the frame\n",
    "        face_results = face_detection.process(rgb_frame)\n",
    "        if face_results.detections:\n",
    "            for i, detection in enumerate(face_results.detections):\n",
    "                # Extract bounding box\n",
    "                bboxC = detection.location_data.relative_bounding_box\n",
    "                ih, iw, _ = frame.shape\n",
    "                x, y, w, h = (int(bboxC.xmin * iw), int(bboxC.ymin * ih),\n",
    "                              int(bboxC.width * iw), int(bboxC.height * ih))\n",
    "                bbox = (x, y, w, h)\n",
    "\n",
    "                # Extract face region\n",
    "                face_roi = rgb_frame[y:y+h, x:x+w]\n",
    "                if face_roi.size == 0:\n",
    "                    continue\n",
    "\n",
    "                try:\n",
    "                    # Generate face embedding\n",
    "                    embedding = DeepFace.represent(face_roi, model_name=\"Facenet\", enforce_detection=False)[0]['embedding']\n",
    "                    person_id = assign_id(embedding, bbox, profiles, frame_count)\n",
    "\n",
    "                    # Draw bounding box and label with embedding ID\n",
    "                    cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "                    cv2.putText(frame, f\"ID: {person_id}\", (x, y - 10),\n",
    "                                cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing face embedding in frame {frame_count}: {e}\")\n",
    "\n",
    "        # Remove stale profiles\n",
    "        profiles = {k: v for k, v in profiles.items() if frame_count - v[\"last_seen\"] <= max_inactive_frames}\n",
    "\n",
    "        # Display the processed frame\n",
    "        cv2.imshow(\"MediaPipe YouTube Video Processing\", frame)\n",
    "\n",
    "        # Adjustable frame delay\n",
    "        time.sleep(frame_delay_ms / 1000.0)  # Convert ms to seconds\n",
    "\n",
    "        # Exit when 'q' is pressed\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            print(\"Exiting video processing.\")\n",
    "            break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "# Save profiles to JSON\n",
    "with open(\"profiles.json\", \"w\") as f:\n",
    "    json.dump(profiles, f, indent=4)\n",
    "print(f\"Profiles saved to profiles.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video URL: https://rr2---sn-o097znsr.googlevideo.com/videoplayback?expire=1733935790&ei=Tm5ZZ8OCHvr-sfIP1OPhoAg&ip=2601%3A644%3A601%3Afa0%3A18f1%3A6d9c%3A9455%3Ae968&id=o-AM0VWy9id_RhIja_CuAX_7jCOsmuB3nF0M_O12ZHnnTH&itag=18&source=youtube&requiressl=yes&xpc=EgVo2aDSNQ%3D%3D&met=1733914190%2C&mh=Vq&mm=31%2C29&mn=sn-o097znsr%2Csn-n4v7sns7&ms=au%2Crdu&mv=m&mvi=2&pl=34&rms=au%2Cau&initcwndbps=3915000&bui=AQn3pFQKhi7MVAR6il70JWVEwJTRLPZJ7nzHT55pqdE6sdGYDy3fT88Js2ixpJEj8o83imVFwTnhby9Q&vprv=1&svpuc=1&mime=video%2Fmp4&ns=dgnEsjCi-frwcKg6mSObwzwQ&rqh=1&gir=yes&clen=36612911&ratebypass=yes&dur=398.593&lmt=1733879672190924&mt=1733913906&fvip=3&fexp=51326932%2C51331020%2C51335594%2C51347747&c=MWEB&sefc=1&txp=5538434&n=jbygYr9NxynRtw&sparams=expire%2Cei%2Cip%2Cid%2Citag%2Csource%2Crequiressl%2Cxpc%2Cbui%2Cvprv%2Csvpuc%2Cmime%2Cns%2Crqh%2Cgir%2Cclen%2Cratebypass%2Cdur%2Clmt&sig=AJfQdSswRQIgR_sLg4GdoV2OPdKEWhxKh-0u7jr7-NyGlLR3-mI-5a4CIQCMCLTi1P8C5kClsDZoxqTwkw4zeYhfluOoJaAbPTohlA%3D%3D&lsparams=met%2Cmh%2Cmm%2Cmn%2Cms%2Cmv%2Cmvi%2Cpl%2Crms%2Cinitcwndbps&lsig=AGluJ3MwRgIhALgf6xqIvH2_zJFtPeRe5B3pKHJ1_9PBscS31k00XqeSAiEAnM93YZVNr-yM9FGS5GMcczeju1avkIGr7LL3n5C5ztY%3D\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 32\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m     30\u001b[0m         cv2\u001b[38;5;241m.\u001b[39mimshow(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYouTube Video\u001b[39m\u001b[38;5;124m\"\u001b[39m, frame)\n\u001b[0;32m---> 32\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwaitKey\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;241m&\u001b[39m \u001b[38;5;241m0xFF\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mord\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mq\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m     33\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m     35\u001b[0m cap\u001b[38;5;241m.\u001b[39mrelease()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "#old code that just streams without mediapipe facial recognition\n",
    "# Get the direct video URL\n",
    "import yt_dlp\n",
    "import cv2\n",
    "\n",
    "def get_video_url(youtube_url):\n",
    "    ydl_opts = {\n",
    "        'quiet': True,\n",
    "        'format': 'best[ext=mp4]'\n",
    "    }\n",
    "    with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
    "        info = ydl.extract_info(youtube_url, download=False)\n",
    "        return info['url']\n",
    "\n",
    "# Get the direct video URL\n",
    "youtube_url = \"https://www.youtube.com/watch?v=m34ZKJNyxac\"\n",
    "video_url = get_video_url(youtube_url)\n",
    "print(\"Video URL:\", video_url)\n",
    "\n",
    "# Open video stream with OpenCV\n",
    "cap = cv2.VideoCapture(video_url)\n",
    "\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Unable to open video stream.\")\n",
    "else:\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        cv2.imshow(\"YouTube Video\", frame)\n",
    "\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "video_url = get_video_url(youtube_url)\n",
    "print(\"Video URL:\", video_url)\n",
    "\n",
    "# Open video stream with OpenCV\n",
    "cap = cv2.VideoCapture(video_url)\n",
    "\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Unable to open video stream.\")\n",
    "else:\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        cv2.imshow(\"YouTube Video\", frame)\n",
    "\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Todo, need to add cookies for age restricted content"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "office",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
