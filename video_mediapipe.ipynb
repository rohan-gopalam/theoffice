{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello world\n"
     ]
    }
   ],
   "source": [
    "print(\"hello world\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded to detector.tflite\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "url = \"https://storage.googleapis.com/mediapipe-models/face_detector/blaze_face_short_range/float16/1/blaze_face_short_range.tflite\"\n",
    "output_path = \"detector.tflite\"\n",
    "\n",
    "response = requests.get(url)\n",
    "if response.status_code == 200:\n",
    "    with open(output_path, \"wb\") as f:\n",
    "        f.write(response.content)\n",
    "    print(f\"Downloaded to {output_path}\")\n",
    "else:\n",
    "    print(f\"Failed to download. HTTP Status Code: {response.status_code}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-18 23:33:07.257365: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1734593598.761831 3453900 gl_context.cc:357] GL version: 2.1 (2.1 ATI-4.14.1), renderer: AMD Radeon Pro 555 OpenGL Engine\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "W0000 00:00:1734593598.795860 3456626 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1734593598.825625 3453900 gl_context.cc:357] GL version: 2.1 (2.1 ATI-4.14.1), renderer: AMD Radeon Pro 555 OpenGL Engine\n",
      "W0000 00:00:1734593599.282087 3456791 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1734593599.381411 3456795 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video URL: https://rr4---sn-jxopj-n5oe.googlevideo.com/videoplayback?expire=1734615201&ei=QcxjZ9a7MrC_sfIPmcW6mAg&ip=2607%3Af140%3A400%3A6b%3A8481%3Aaa15%3A119c%3Ac39f&id=o-AHMtIE2B7rt86xcjQJ74X5F3mHkVBszdTBqUfYH2_EoT&itag=18&source=youtube&requiressl=yes&xpc=EgVo2aDSNQ%3D%3D&met=1734593601%2C&mh=KN&mm=31%2C29&mn=sn-jxopj-n5oe%2Csn-n4v7snls&ms=au%2Crdu&mv=m&mvi=4&pl=32&rms=au%2Cau&initcwndbps=5768750&bui=AfMhrI9A5NVVLnMo-BCaK2c4dDcabu4rmNZfaRaFIqUwFRnHzBjf50n_2jAyGNV1apB6Srpu07tVYUmP&vprv=1&svpuc=1&mime=video%2Fmp4&ns=DDY6Y3snuKkYblvJbwsvXHIQ&rqh=1&cnr=14&ratebypass=yes&dur=5104.187&lmt=1729255083328319&mt=1734593377&fvip=1&fexp=51326932%2C51331020%2C51335594%2C51371293&c=MWEB&sefc=1&txp=5538434&n=2Xa3z-X7KImdWw&sparams=expire%2Cei%2Cip%2Cid%2Citag%2Csource%2Crequiressl%2Cxpc%2Cbui%2Cvprv%2Csvpuc%2Cmime%2Cns%2Crqh%2Ccnr%2Cratebypass%2Cdur%2Clmt&sig=AJfQdSswRAIgaEgtD658MA7VNtYs6yY6FpRK8joQGSzUEnObec79hoACIGylz7ddkCa1ZlUHwXKmKU1AgP73gNdrFfge501UjpTB&lsparams=met%2Cmh%2Cmm%2Cmn%2Cms%2Cmv%2Cmvi%2Cpl%2Crms%2Cinitcwndbps&lsig=AGluJ3MwRgIhAKw72VjqruKwAv74a0PT7eAq42QzAxRzfGMrWkQ49JP-AiEA8WwUUe8gIR5yr6yrtP8xRukbXwV89nHl_ZZF4nTr898%3D\n",
      "Processing YouTube video. Press 'q' to quit.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1734593605.258694 3456789 landmark_projection_calculator.cc:186] Using NORM_RECT without IMAGE_DIMENSIONS is only supported for the square ROI. Provide IMAGE_DIMENSIONS or use PROJECTION_MATRIX.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frame 40: Assigned Person ID 1\n",
      "Frame 45: Assigned Person ID 2\n",
      "Frame 65: Assigned Person ID 3\n",
      "Frame 85: Assigned Person ID 2\n",
      "Frame 90: Assigned Person ID 3\n",
      "Frame 95: Assigned Person ID 3\n",
      "Frame 100: Assigned Person ID 3\n",
      "Frame 100: Assigned Person ID 3\n",
      "Frame 105: Assigned Person ID 3\n",
      "Frame 105: Assigned Person ID 3\n",
      "Frame 110: Assigned Person ID 3\n",
      "Frame 110: Assigned Person ID 3\n",
      "Frame 115: Assigned Person ID 3\n",
      "Frame 120: Assigned Person ID 3\n",
      "Frame 125: Assigned Person ID 2\n",
      "Frame 130: Assigned Person ID 3\n",
      "Frame 135: Assigned Person ID 3\n",
      "Frame 140: Assigned Person ID 3\n",
      "Frame 185: Assigned Person ID 1\n",
      "Frame 225: Assigned Person ID 1\n",
      "Frame 230: Assigned Person ID 2\n",
      "Frame 235: Assigned Person ID 3\n",
      "Frame 240: Assigned Person ID 4\n",
      "Frame 245: Assigned Person ID 5\n",
      "Frame 250: Assigned Person ID 6\n",
      "Frame 255: Assigned Person ID 7\n",
      "Exiting video processing.\n",
      "Profiles saved to profiles.json\n",
      "Processed 255 frames in 21.69 seconds.\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import json\n",
    "import time\n",
    "from deepface import DeepFace\n",
    "from yt_dlp import YoutubeDL\n",
    "from scipy.spatial.distance import euclidean\n",
    "\n",
    "# Initialize MediaPipe solutions\n",
    "mp_face_detection = mp.solutions.face_detection\n",
    "mp_pose = mp.solutions.pose\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "face_detection = mp_face_detection.FaceDetection(min_detection_confidence=0.5)\n",
    "pose = mp_pose.Pose(min_detection_confidence=0.5)\n",
    "\n",
    "# Profiles for tracking people\n",
    "profiles = {}\n",
    "max_inactive_frames = 30  # Maximum frames to keep a profile if not detected\n",
    "\n",
    "# Function to assign or update ID based on face embedding and spatial proximity\n",
    "def assign_id(embedding, bbox, profiles, threshold=0.6, spatial_threshold=50):\n",
    "    best_match_id = None\n",
    "    best_similarity = float(\"inf\")\n",
    "    best_distance = float(\"inf\")\n",
    "\n",
    "    bbox_center = ((bbox[0] + bbox[2] / 2), (bbox[1] + bbox[3] / 2))\n",
    "\n",
    "    for profile_id, profile in profiles.items():\n",
    "        # Compare embedding similarity\n",
    "        similarity = np.linalg.norm(np.array(profile['face_embedding']) - np.array(embedding))\n",
    "        # Compare spatial proximity\n",
    "        profile_bbox = profile[\"bbox\"]\n",
    "        profile_center = ((profile_bbox[0] + profile_bbox[2] / 2), (profile_bbox[1] + profile_bbox[3] / 2))\n",
    "        spatial_distance = euclidean(profile_center, bbox_center)\n",
    "\n",
    "        # Find the best match\n",
    "        if similarity < threshold and spatial_distance < spatial_threshold:\n",
    "            if similarity < best_similarity or (similarity == best_similarity and spatial_distance < best_distance):\n",
    "                best_match_id = profile_id\n",
    "                best_similarity = similarity\n",
    "                best_distance = spatial_distance\n",
    "\n",
    "    if best_match_id is not None:\n",
    "        # Update the matched profile\n",
    "        profiles[best_match_id][\"bbox\"] = bbox\n",
    "        profiles[best_match_id][\"last_seen\"] = frame_count\n",
    "        return best_match_id\n",
    "    else:\n",
    "        # Create a new profile\n",
    "        new_id = len(profiles) + 1\n",
    "        profiles[new_id] = {\"face_embedding\": embedding, \"actions\": [], \"gaze\": {}, \"pose\": {}, \"bbox\": bbox, \"last_seen\": frame_count}\n",
    "        return new_id\n",
    "\n",
    "# Function to fetch video URL using yt-dlp\n",
    "def get_video_url(youtube_url):\n",
    "    ydl_opts = {'quiet': True, 'format': 'best[ext=mp4]'}\n",
    "    with YoutubeDL(ydl_opts) as ydl:\n",
    "        info = ydl.extract_info(youtube_url, download=False)\n",
    "        return info['url']\n",
    "\n",
    "# Process a YouTube video\n",
    "youtube_url = \"https://www.youtube.com/watch?v=96Y6mc3C1Bg\"  # Replace with your URL\n",
    "video_url = get_video_url(youtube_url)\n",
    "print(\"Video URL:\", video_url)\n",
    "\n",
    "cap = cv2.VideoCapture(video_url)\n",
    "\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Unable to open the YouTube video stream.\")\n",
    "else:\n",
    "    print(\"Processing YouTube video. Press 'q' to quit.\")\n",
    "    frame_count = 0\n",
    "    start_time = time.time()\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            print(\"End of video or error reading the video stream.\")\n",
    "            break\n",
    "\n",
    "        # Skip frames for efficiency\n",
    "        if frame_count % 5 != 0:\n",
    "            frame_count += 1\n",
    "            continue\n",
    "\n",
    "        # Validate frame dimensions\n",
    "        if frame is None or frame.shape[0] == 0 or frame.shape[1] == 0:\n",
    "            print(f\"Skipped frame {frame_count}: invalid dimensions.\")\n",
    "            continue\n",
    "\n",
    "        # Get frame dimensions\n",
    "        ih, iw, _ = frame.shape\n",
    "\n",
    "        # Convert frame to RGB for MediaPipe\n",
    "        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Face Detection\n",
    "        face_results = face_detection.process(rgb_frame)\n",
    "        detected_person_ids = []\n",
    "        if face_results.detections:\n",
    "            for detection in face_results.detections:\n",
    "                bboxC = detection.location_data.relative_bounding_box\n",
    "                bbox = int(bboxC.xmin * iw), int(bboxC.ymin * ih), int(bboxC.width * iw), int(bboxC.height * ih)\n",
    "                \n",
    "                # Extract face region\n",
    "                x, y, w, h = bbox\n",
    "                face_roi = rgb_frame[y:y+h, x:x+w]\n",
    "\n",
    "                try:\n",
    "                    # Generate face embedding\n",
    "                    embedding = DeepFace.represent(face_roi, model_name=\"Facenet\", enforce_detection=False)[0]['embedding']\n",
    "                    person_id = assign_id(embedding, bbox, profiles)\n",
    "                    detected_person_ids.append(person_id)\n",
    "\n",
    "                    # Debugging: Log assigned person ID\n",
    "                    print(f\"Frame {frame_count}: Assigned Person ID {person_id}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing face embedding in frame {frame_count}: {e}\")\n",
    "                    continue\n",
    "\n",
    "        # Pose Detection\n",
    "        pose_results = pose.process(rgb_frame)\n",
    "        if pose_results.pose_landmarks:\n",
    "            mp_drawing.draw_landmarks(frame, pose_results.pose_landmarks, mp_pose.POSE_CONNECTIONS)\n",
    "\n",
    "            # Assign pose landmarks to profiles\n",
    "            for lm in pose_results.pose_landmarks.landmark:\n",
    "                pose_coords = (lm.x * iw, lm.y * ih)\n",
    "                # Find the nearest face and associate\n",
    "                for person_id in detected_person_ids:\n",
    "                    profiles[person_id][\"pose\"] = {\"landmarks\": pose_coords}\n",
    "\n",
    "        # Remove stale profiles\n",
    "        profiles = {k: v for k, v in profiles.items() if frame_count - v[\"last_seen\"] <= max_inactive_frames}\n",
    "\n",
    "        # Debugging: Draw bounding boxes and IDs on the frame for visualization\n",
    "        for person_id, profile in profiles.items():\n",
    "            bbox = profile[\"bbox\"]\n",
    "            x, y, w, h = bbox\n",
    "            cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "            cv2.putText(frame, f\"ID: {person_id}\", (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 1)\n",
    "\n",
    "        # Display frame\n",
    "        cv2.imshow(\"MediaPipe YouTube Video Processing\", frame)\n",
    "\n",
    "        # Save profiles every 100 frames\n",
    "        if frame_count % 100 == 0:\n",
    "            with open(\"profiles_temp.json\", \"w\") as f:\n",
    "                json.dump(profiles, f, indent=4)\n",
    "\n",
    "        # Exit on 'q' key press\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            print(\"Exiting video processing.\")\n",
    "            break\n",
    "\n",
    "        frame_count += 1\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "# Final Save: Save profiles to JSON\n",
    "with open(\"profiles.json\", \"w\") as f:\n",
    "    json.dump(profiles, f, indent=4)\n",
    "print(f\"Profiles saved to profiles.json\")\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Processed {frame_count} frames in {end_time - start_time:.2f} seconds.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video URL: https://rr2---sn-o097znsr.googlevideo.com/videoplayback?expire=1733935790&ei=Tm5ZZ8OCHvr-sfIP1OPhoAg&ip=2601%3A644%3A601%3Afa0%3A18f1%3A6d9c%3A9455%3Ae968&id=o-AM0VWy9id_RhIja_CuAX_7jCOsmuB3nF0M_O12ZHnnTH&itag=18&source=youtube&requiressl=yes&xpc=EgVo2aDSNQ%3D%3D&met=1733914190%2C&mh=Vq&mm=31%2C29&mn=sn-o097znsr%2Csn-n4v7sns7&ms=au%2Crdu&mv=m&mvi=2&pl=34&rms=au%2Cau&initcwndbps=3915000&bui=AQn3pFQKhi7MVAR6il70JWVEwJTRLPZJ7nzHT55pqdE6sdGYDy3fT88Js2ixpJEj8o83imVFwTnhby9Q&vprv=1&svpuc=1&mime=video%2Fmp4&ns=dgnEsjCi-frwcKg6mSObwzwQ&rqh=1&gir=yes&clen=36612911&ratebypass=yes&dur=398.593&lmt=1733879672190924&mt=1733913906&fvip=3&fexp=51326932%2C51331020%2C51335594%2C51347747&c=MWEB&sefc=1&txp=5538434&n=jbygYr9NxynRtw&sparams=expire%2Cei%2Cip%2Cid%2Citag%2Csource%2Crequiressl%2Cxpc%2Cbui%2Cvprv%2Csvpuc%2Cmime%2Cns%2Crqh%2Cgir%2Cclen%2Cratebypass%2Cdur%2Clmt&sig=AJfQdSswRQIgR_sLg4GdoV2OPdKEWhxKh-0u7jr7-NyGlLR3-mI-5a4CIQCMCLTi1P8C5kClsDZoxqTwkw4zeYhfluOoJaAbPTohlA%3D%3D&lsparams=met%2Cmh%2Cmm%2Cmn%2Cms%2Cmv%2Cmvi%2Cpl%2Crms%2Cinitcwndbps&lsig=AGluJ3MwRgIhALgf6xqIvH2_zJFtPeRe5B3pKHJ1_9PBscS31k00XqeSAiEAnM93YZVNr-yM9FGS5GMcczeju1avkIGr7LL3n5C5ztY%3D\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 32\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m     30\u001b[0m         cv2\u001b[38;5;241m.\u001b[39mimshow(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYouTube Video\u001b[39m\u001b[38;5;124m\"\u001b[39m, frame)\n\u001b[0;32m---> 32\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwaitKey\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;241m&\u001b[39m \u001b[38;5;241m0xFF\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mord\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mq\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m     33\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m     35\u001b[0m cap\u001b[38;5;241m.\u001b[39mrelease()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "#old code that just streams without mediapipe facial recognition\n",
    "# Get the direct video URL\n",
    "import yt_dlp\n",
    "import cv2\n",
    "\n",
    "def get_video_url(youtube_url):\n",
    "    ydl_opts = {\n",
    "        'quiet': True,\n",
    "        'format': 'best[ext=mp4]'\n",
    "    }\n",
    "    with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
    "        info = ydl.extract_info(youtube_url, download=False)\n",
    "        return info['url']\n",
    "\n",
    "# Get the direct video URL\n",
    "youtube_url = \"https://www.youtube.com/watch?v=m34ZKJNyxac\"\n",
    "video_url = get_video_url(youtube_url)\n",
    "print(\"Video URL:\", video_url)\n",
    "\n",
    "# Open video stream with OpenCV\n",
    "cap = cv2.VideoCapture(video_url)\n",
    "\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Unable to open video stream.\")\n",
    "else:\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        cv2.imshow(\"YouTube Video\", frame)\n",
    "\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "video_url = get_video_url(youtube_url)\n",
    "print(\"Video URL:\", video_url)\n",
    "\n",
    "# Open video stream with OpenCV\n",
    "cap = cv2.VideoCapture(video_url)\n",
    "\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Unable to open video stream.\")\n",
    "else:\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        cv2.imshow(\"YouTube Video\", frame)\n",
    "\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Todo, need to add cookies for age restricted content"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "office",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
