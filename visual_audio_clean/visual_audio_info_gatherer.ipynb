{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import json\n",
    "from deepface import DeepFace\n",
    "from scipy.spatial.distance import cosine\n",
    "from ultralytics import YOLO\n",
    "from sklearn.cluster import DBSCAN  # new import for clustering\n",
    "\n",
    "# -----------------------------\n",
    "# Configuration and Constants\n",
    "# -----------------------------\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Transform for gaze detection model\n",
    "gaze_transform = transforms.Compose([\n",
    "    transforms.Resize((448, 448)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Threshold for matching face embeddings across frames (used initially)\n",
    "EMBEDDING_THRESHOLD = 0.6\n",
    "# Confidence threshold for YOLO face detection\n",
    "YOLO_CONF_THRESHOLD = 0.5\n",
    "\n",
    "# Flag to show YOLO detections per frame visually\n",
    "SHOW_YOLO_DETECTIONS = True\n",
    "\n",
    "# -----------------------------\n",
    "# Custom JSON Encoder\n",
    "# -----------------------------\n",
    "class NumpyEncoder(json.JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, (np.integer)):\n",
    "            return int(obj)\n",
    "        elif isinstance(obj, (np.floating)):\n",
    "            return float(obj)\n",
    "        elif isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "        return super(NumpyEncoder, self).default(obj)\n",
    "\n",
    "# -----------------------------\n",
    "# YOLO Face Detection Function\n",
    "# -----------------------------\n",
    "def detect_faces_yolo(img_array, yolo_face_model, conf_threshold=YOLO_CONF_THRESHOLD):\n",
    "    \"\"\"\n",
    "    Detect faces in an image using YOLO.\n",
    "    \n",
    "    Args:\n",
    "        img_array: Input image as a NumPy array.\n",
    "        yolo_face_model: Loaded YOLO model.\n",
    "        conf_threshold: Confidence threshold.\n",
    "    \n",
    "    Returns:\n",
    "        List of bounding boxes [x1, y1, x2, y2] in pixel coordinates.\n",
    "    \"\"\"\n",
    "    results = yolo_face_model(img_array)\n",
    "    face_boxes = []\n",
    "    if results and results[0].boxes is not None:\n",
    "        boxes = results[0].boxes.xyxy.cpu().numpy()  # [N,4]\n",
    "        confs = results[0].boxes.conf.cpu().numpy()    # [N]\n",
    "        for box, conf in zip(boxes, confs):\n",
    "            if conf >= conf_threshold:\n",
    "                face_boxes.append([int(x) for x in box])\n",
    "    return face_boxes\n",
    "\n",
    "# -----------------------------\n",
    "# (Optional) Known Faces Loading (Profiles)\n",
    "# -----------------------------\n",
    "def load_known_faces(image_paths, yolo_face_model):\n",
    "    \"\"\"\n",
    "    Load known faces (profiles) using YOLO for detection and DeepFace for embeddings.\n",
    "    \n",
    "    Args:\n",
    "        image_paths: Dictionary {name: image_path}\n",
    "        yolo_face_model: Loaded YOLO model.\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary {name: embedding} for known people.\n",
    "    \"\"\"\n",
    "    known_faces = {}\n",
    "    for name, path in image_paths.items():\n",
    "        try:\n",
    "            img = cv2.imread(path)\n",
    "            if img is None:\n",
    "                print(f\"Could not load image for {name} at {path}\")\n",
    "                continue\n",
    "            detected = detect_faces_yolo(img, yolo_face_model)\n",
    "            if detected:\n",
    "                x1, y1, x2, y2 = detected[0]\n",
    "                face_roi = img[y1:y2, x1:x2]\n",
    "                rep = DeepFace.represent(face_roi, model_name='Facenet512',\n",
    "                                         detector_backend='skip', enforce_detection=False)\n",
    "                embedding = np.array(rep[0]['embedding'])\n",
    "                known_faces[name] = embedding\n",
    "                print(f\"Successfully loaded face for {name}\")\n",
    "            else:\n",
    "                print(f\"No face detected for {name} in {path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading face for {name}: {e}\")\n",
    "    return known_faces\n",
    "\n",
    "# -----------------------------\n",
    "# Face Embedding and Matching\n",
    "# -----------------------------\n",
    "def get_face_embedding(face_roi):\n",
    "    \"\"\"Extract face embedding using DeepFace (detection skipped).\"\"\"\n",
    "    try:\n",
    "        rep = DeepFace.represent(face_roi, model_name='Facenet512',\n",
    "                                 detector_backend='skip', enforce_detection=False)\n",
    "        return np.array(rep[0]['embedding'])\n",
    "    except Exception as e:\n",
    "        print(f\"Embedding error: {e}\")\n",
    "        return None\n",
    "\n",
    "def match_known_face(face_emb, known_faces):\n",
    "    \"\"\"\n",
    "    Match a face embedding to the most similar known face.\n",
    "    \n",
    "    Args:\n",
    "        face_emb: Detected face embedding.\n",
    "        known_faces: Dictionary {name: embedding}.\n",
    "    \n",
    "    Returns:\n",
    "        Tuple (name, similarity_score)\n",
    "    \"\"\"\n",
    "    best_match = None\n",
    "    best_score = float('inf')\n",
    "    for kname, known_emb in known_faces.items():\n",
    "        dist = cosine(known_emb, face_emb)\n",
    "        if dist < best_score:\n",
    "            best_score = dist\n",
    "            best_match = kname\n",
    "    return best_match, best_score\n",
    "\n",
    "def analyze_emotions(face_roi):\n",
    "    \"\"\"\n",
    "    Analyze emotions for the given face ROI using DeepFace.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        analysis = DeepFace.analyze(face_roi, actions=['emotion'],\n",
    "                                    detector_backend='skip', enforce_detection=False)\n",
    "        return analysis[0]['dominant_emotion']\n",
    "    except Exception as e:\n",
    "        print(f\"Emotion analysis error: {e}\")\n",
    "        return \"Unknown\"\n",
    "\n",
    "# -----------------------------\n",
    "# Visualization Function\n",
    "# -----------------------------\n",
    "def visualize_all(pil_image, heatmaps, bboxes, inout_scores=None, emotions=None, names=None, inout_thresh=0.5):\n",
    "    \"\"\"\n",
    "    Create a visualization image with bounding boxes, gaze points, and header text.\n",
    "    \n",
    "    Args:\n",
    "        pil_image: PIL Image.\n",
    "        heatmaps: Heatmaps for each face.\n",
    "        bboxes: Normalized bounding boxes (values in [0, 1]).\n",
    "        inout_scores: In-out scores for gaze (optional).\n",
    "        emotions: Emotion strings (optional).\n",
    "        names: Face names (optional).\n",
    "        inout_thresh: Threshold for considering gaze as \"looking at camera\".\n",
    "    \n",
    "    Returns:\n",
    "        An annotated PIL Image.\n",
    "    \"\"\"\n",
    "    colors = ['lime', 'tomato', 'cyan', 'fuchsia', 'yellow']\n",
    "    width, height = pil_image.size\n",
    "    header_height = 100\n",
    "    output_img = Image.new(\"RGBA\", (width, height + header_height), (255,255,255,255))\n",
    "    output_img.paste(pil_image, (0, header_height))\n",
    "    draw = ImageDraw.Draw(output_img)\n",
    "    draw.rectangle([0, 0, width, header_height], fill=(240,240,240,255))\n",
    "    draw.line([(0, header_height), (width, header_height)], fill=(100,100,100,255), width=2)\n",
    "    header_text = []\n",
    "    for i, norm_box in enumerate(bboxes):\n",
    "        xmin, ymin, xmax, ymax = norm_box\n",
    "        xmin_px = xmin * width\n",
    "        ymin_px = ymin * height + header_height\n",
    "        xmax_px = xmax * width\n",
    "        ymax_px = ymax * height + header_height\n",
    "        color = colors[i % len(colors)]\n",
    "        line_width = max(1, int(min(width, height) * 0.01))\n",
    "        draw.rectangle([xmin_px, ymin_px, xmax_px, ymax_px], outline=color, width=line_width)\n",
    "        info_list = []\n",
    "        if names is not None and i < len(names):\n",
    "            info_list.append(f\"{names[i]}\")\n",
    "        else:\n",
    "            info_list.append(f\"Person {i+1}\")\n",
    "        if emotions is not None and i < len(emotions):\n",
    "            info_list.append(f\"Emotion: {emotions[i]}\")\n",
    "        if inout_scores is not None and i < len(inout_scores):\n",
    "            score_val = float(inout_scores[i].item() if torch.is_tensor(inout_scores[i]) else inout_scores[i])\n",
    "            look_str = \"Looking at camera\" if score_val > inout_thresh else \"Not looking at camera\"\n",
    "            info_list.append(f\"{look_str} ({score_val:.2f})\")\n",
    "        header_text.append((info_list, color))\n",
    "        if heatmaps is not None and i < len(heatmaps):\n",
    "            do_draw = True\n",
    "            if inout_scores is not None and i < len(inout_scores):\n",
    "                score_val = float(inout_scores[i].item() if torch.is_tensor(inout_scores[i]) else inout_scores[i])\n",
    "                do_draw = score_val > inout_thresh\n",
    "            if do_draw:\n",
    "                heat_np = heatmaps[i].detach().cpu().numpy() if torch.is_tensor(heatmaps[i]) else heatmaps[i]\n",
    "                max_idx = np.unravel_index(np.argmax(heat_np), heat_np.shape)\n",
    "                gaze_y, gaze_x = max_idx\n",
    "                gaze_x = gaze_x / heat_np.shape[1] * width\n",
    "                gaze_y = gaze_y / heat_np.shape[0] * height + header_height\n",
    "                center_x = ((xmin + xmax) / 2) * width\n",
    "                center_y = ((ymin + ymax) / 2) * height + header_height\n",
    "                radius = max(1, int(min(width, height) * 0.01))\n",
    "                draw.ellipse([(gaze_x - radius, gaze_y - radius), (gaze_x + radius, gaze_y + radius)], fill=color)\n",
    "                draw.line([(center_x, center_y), (gaze_x, gaze_y)], fill=color, width=line_width)\n",
    "    font_size = 24\n",
    "    try:\n",
    "        font = ImageFont.truetype(\"Arial\", font_size)\n",
    "    except:\n",
    "        font = ImageFont.load_default()\n",
    "    y_offset = 10\n",
    "    for info, col in header_text:\n",
    "        box_size = 15\n",
    "        draw.rectangle([10, y_offset, 10 + box_size, y_offset + box_size], fill=col)\n",
    "        draw.text((10 + box_size + 5, y_offset), \" | \".join(info), fill=\"black\", font=font)\n",
    "        y_offset += font_size + 5\n",
    "    title = f\"Analysis for {len(bboxes)} detected face(s)\"\n",
    "    title_font_size = 32\n",
    "    try:\n",
    "        title_font = ImageFont.truetype(\"Arial\", title_font_size)\n",
    "    except:\n",
    "        title_font = font\n",
    "    title_width = title_font.getlength(title) if title_font else len(title) * title_font_size * 0.6\n",
    "    title_x = (width - title_width) / 2\n",
    "    draw.text((title_x, 0), title, fill=\"black\", font=title_font)\n",
    "    return output_img\n",
    "\n",
    "# -----------------------------\n",
    "# Visualize YOLO Detections Function\n",
    "# -----------------------------\n",
    "def visualize_yolo_detections(img_array, yolo_face_model, save_path=None):\n",
    "    \"\"\"\n",
    "    Use the YOLO model's built-in plotting method to create an annotated image.\n",
    "    \n",
    "    Args:\n",
    "        img_array: Input image as a NumPy array.\n",
    "        yolo_face_model: Loaded YOLO model.\n",
    "        save_path: Optional path to save the annotated image.\n",
    "    \n",
    "    Returns:\n",
    "        Annotated image as a NumPy array.\n",
    "    \"\"\"\n",
    "    results = yolo_face_model(img_array)\n",
    "    annotated_img = results[0].plot()  # Get annotated image from the first result\n",
    "    if save_path:\n",
    "        cv2.imwrite(save_path, annotated_img)\n",
    "    return annotated_img\n",
    "\n",
    "# -----------------------------\n",
    "# Function to Reclustering Profiles Using DBSCAN\n",
    "# -----------------------------\n",
    "def recluster_profiles(profiles, eps=0.2):\n",
    "    \"\"\"\n",
    "    Recluster the collected profile embeddings using DBSCAN to separate distinct individuals.\n",
    "    \n",
    "    Args:\n",
    "        profiles: Dictionary mapping profile IDs to dicts that contain at least an \"embedding\" and \"frames_seen\".\n",
    "        eps: The epsilon parameter for DBSCAN (in cosine distance).\n",
    "    \n",
    "    Returns:\n",
    "        new_profiles: Dictionary with new profile IDs and aggregated frame appearances.\n",
    "        cluster_map: Dictionary mapping old profile IDs to new cluster labels.\n",
    "    \"\"\"\n",
    "    old_ids = []\n",
    "    embeddings = []\n",
    "    for pid, prof in profiles.items():\n",
    "        old_ids.append(pid)\n",
    "        embeddings.append(prof[\"embedding\"])\n",
    "    embeddings = np.array(embeddings)\n",
    "    db = DBSCAN(eps=eps, min_samples=1, metric='cosine').fit(embeddings)\n",
    "    labels = db.labels_\n",
    "    cluster_map = {old: label for old, label in zip(old_ids, labels)}\n",
    "    new_profiles = {}\n",
    "    for old_pid, label in cluster_map.items():\n",
    "        if label in new_profiles:\n",
    "            new_profiles[label][\"frames_seen\"].extend(profiles[old_pid][\"frames_seen\"])\n",
    "        else:\n",
    "            new_profiles[label] = {\n",
    "                \"embedding\": profiles[old_pid][\"embedding\"],\n",
    "                \"name\": f\"Person {label + 1}\",\n",
    "                \"frames_seen\": profiles[old_pid][\"frames_seen\"].copy()\n",
    "            }\n",
    "    return new_profiles, cluster_map\n",
    "\n",
    "# -----------------------------\n",
    "# Create JSON for LLM Analysis (without id field)\n",
    "# -----------------------------\n",
    "def create_llm_input(results, output_path=\"llm_analysis_input.json\"):\n",
    "    \"\"\"\n",
    "    Convert analysis results to a structured JSON file without including an 'id' field for persons.\n",
    "    \n",
    "    Args:\n",
    "        results: Analysis result dictionary.\n",
    "        output_path: Path to save the JSON file.\n",
    "        \n",
    "    Returns:\n",
    "        The structured data dictionary.\n",
    "    \"\"\"\n",
    "    llm_data = {\n",
    "        \"session_summary\": {\n",
    "            \"total_frames\": len([k for k in results.keys() if k not in ['profiles', 'visualizations']]),\n",
    "            \"people_detected\": {}\n",
    "        },\n",
    "        \"frames\": []\n",
    "    }\n",
    "    profiles = results.get('profiles', {})\n",
    "    for pid, prof in profiles.items():\n",
    "        # In session summary, we keep only the person name and frames_seen.\n",
    "        llm_data[\"session_summary\"][\"people_detected\"][str(pid)] = {\n",
    "            \"name\": prof['name'],\n",
    "            \"frame_appearances\": prof['frames_seen']\n",
    "        }\n",
    "    for frame_path, frame_data in results.items():\n",
    "        if frame_path in ['profiles', 'visualizations']:\n",
    "            continue\n",
    "        entry = {\"frame_path\": frame_path, \"people\": []}\n",
    "        for face in frame_data.get('faces', []):\n",
    "            # Note: Remove the \"id\" field from each person.\n",
    "            person = {\n",
    "                \"name\": face.get('name', \"Unknown\"),\n",
    "                \"emotion\": face.get('emotion', \"Unknown\"),\n",
    "                \"position\": {\n",
    "                    \"x1\": int(face['bbox'][0]),\n",
    "                    \"y1\": int(face['bbox'][1]),\n",
    "                    \"x2\": int(face['bbox'][2]),\n",
    "                    \"y2\": int(face['bbox'][3])\n",
    "                }\n",
    "            }\n",
    "            gaze_info = face.get('gaze', {})\n",
    "            if gaze_info:\n",
    "                person[\"gaze\"] = {\n",
    "                    \"looking_at_camera\": gaze_info.get(\"looking_at_camera\", False),\n",
    "                    \"confidence\": float(gaze_info.get(\"inout_score\", 0))\n",
    "                }\n",
    "                heat = gaze_info.get(\"heatmap\")\n",
    "                if heat:\n",
    "                    heat_np = np.array(heat)\n",
    "                    max_idx = np.unravel_index(np.argmax(heat_np), heat_np.shape)\n",
    "                    norm_x = float(max_idx[1]) / heat_np.shape[1]\n",
    "                    norm_y = float(max_idx[0]) / heat_np.shape[0]\n",
    "                    person[\"gaze\"][\"target\"] = {\"x\": norm_x, \"y\": norm_y}\n",
    "            entry[\"people\"].append(person)\n",
    "        if entry[\"people\"]:\n",
    "            summaries = []\n",
    "            for person in entry[\"people\"]:\n",
    "                summ = f\"{person['name']} shows {person['emotion']} emotion\"\n",
    "                if \"gaze\" in person:\n",
    "                    if person[\"gaze\"].get(\"looking_at_camera\", False):\n",
    "                        summ += \" and is looking at the camera\"\n",
    "                    else:\n",
    "                        summ += \" and is not looking at the camera\"\n",
    "                summaries.append(summ)\n",
    "            entry[\"natural_language_summary\"] = \". \".join(summaries) + \".\"\n",
    "        else:\n",
    "            entry[\"natural_language_summary\"] = \"No people detected in this frame.\"\n",
    "        llm_data[\"frames\"].append(entry)\n",
    "    with open(output_path, 'w') as f:\n",
    "        json.dump(llm_data, f, indent=2, cls=NumpyEncoder)\n",
    "    print(f\"LLM analysis input saved to {output_path}\")\n",
    "    preview = json.dumps(llm_data, indent=2, cls=NumpyEncoder)\n",
    "    print(preview[:1000] + \"...\\n\" if len(preview) > 1000 else preview)\n",
    "    return llm_data\n",
    "\n",
    "# -----------------------------\n",
    "# Main Analysis Function (per frame)\n",
    "# -----------------------------\n",
    "def analyze_frames(image_paths, gaze_model, yolo_face_model, known_faces=None, output_dir=None, llm_output_path=\"llm_analysis_input.json\"):\n",
    "    profiles = {}\n",
    "    results = {}\n",
    "    img_tensors = []\n",
    "    norm_boxes_all = []\n",
    "    frame_faces_all = []\n",
    "    print(f\"Processing {len(image_paths)} frames...\")\n",
    "    for idx, path in enumerate(image_paths):\n",
    "        print(f\"Processing frame {idx+1}/{len(image_paths)}: {path}\")\n",
    "        try:\n",
    "            pil_image = Image.open(path).convert(\"RGB\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error opening {path}: {e}\")\n",
    "            continue\n",
    "        width, height = pil_image.size\n",
    "        np_img = np.array(pil_image)\n",
    "        cv_img = cv2.cvtColor(np_img, cv2.COLOR_RGB2BGR)\n",
    "        \n",
    "        # --- YOLO Detection Step ---\n",
    "        detected = detect_faces_yolo(np_img, yolo_face_model)\n",
    "        print(f\"YOLO detections for frame {idx+1} ({path}):\")\n",
    "        if detected:\n",
    "            for j, bbox in enumerate(detected):\n",
    "                print(f\"  Detection {j+1}: Bounding box = {bbox}\")\n",
    "        else:\n",
    "            print(\"  No faces detected by YOLO.\")\n",
    "        \n",
    "        if SHOW_YOLO_DETECTIONS:\n",
    "            annotated_img = visualize_yolo_detections(np_img, yolo_face_model)\n",
    "            plt.figure(figsize=(8, 6))\n",
    "            plt.imshow(cv2.cvtColor(annotated_img, cv2.COLOR_BGR2RGB))\n",
    "            plt.title(f\"YOLO Detections for Frame {idx+1}\")\n",
    "            plt.axis(\"off\")\n",
    "            plt.show()\n",
    "        \n",
    "        results[path] = {\"faces\": []}\n",
    "        face_list = []\n",
    "        norm_list = []\n",
    "        used_pids = set()\n",
    "        if detected:\n",
    "            for det in detected:\n",
    "                x1, y1, x2, y2 = det\n",
    "                face_roi = cv_img[y1:y2, x1:x2]\n",
    "                if face_roi.size == 0:\n",
    "                    continue\n",
    "                emb = get_face_embedding(face_roi)\n",
    "                if emb is None:\n",
    "                    continue\n",
    "                emotion = analyze_emotions(face_roi)\n",
    "                ident = None\n",
    "                sim_score = None\n",
    "                if known_faces:\n",
    "                    ident, sim_score = match_known_face(emb, known_faces)\n",
    "                    print(f\"Frame {idx}: Matched face with {ident} (score: {sim_score:.4f})\")\n",
    "                pid = None\n",
    "                # Only match profiles not already used in this frame.\n",
    "                for p, prof in profiles.items():\n",
    "                    if p in used_pids:\n",
    "                        continue\n",
    "                    if cosine(prof['embedding'], emb) < EMBEDDING_THRESHOLD:\n",
    "                        pid = p\n",
    "                        break\n",
    "                if pid is None:\n",
    "                    pid = len(profiles) + 1\n",
    "                    profiles[pid] = {\"embedding\": emb, \"name\": ident if ident else f\"Person {pid}\", \"frames_seen\": []}\n",
    "                profiles[pid][\"frames_seen\"].append(idx)\n",
    "                used_pids.add(pid)\n",
    "                if ident and profiles[pid][\"name\"].startswith(\"Person \"):\n",
    "                    profiles[pid][\"name\"] = ident\n",
    "                face_data = {\n",
    "                    \"bbox\": (x1, y1, x2, y2),\n",
    "                    \"profile_id\": pid,\n",
    "                    \"identity\": ident,\n",
    "                    \"similarity_score\": sim_score,\n",
    "                    \"emotion\": emotion,\n",
    "                    \"name\": profiles[pid][\"name\"]\n",
    "                }\n",
    "                face_list.append(face_data)\n",
    "                norm_box = np.array([x1/width, y1/height, x2/width, y2/height])\n",
    "                norm_list.append(norm_box)\n",
    "        else:\n",
    "            norm_list = []\n",
    "        norm_boxes_all.append(norm_list)\n",
    "        frame_faces_all.append(face_list)\n",
    "        tensor_img = gaze_transform(pil_image).unsqueeze(0).to(device)\n",
    "        img_tensors.append(tensor_img)\n",
    "    \n",
    "    gaze_out = None\n",
    "    if img_tensors and any(len(b) > 0 for b in norm_boxes_all):\n",
    "        img_batch = torch.cat(img_tensors, dim=0)\n",
    "        inp = {\"images\": img_batch, \"bboxes\": norm_boxes_all}\n",
    "        print(\"Running gaze detection model...\")\n",
    "        with torch.no_grad():\n",
    "            gaze_out = gaze_model(inp)\n",
    "    \n",
    "    visuals = {}\n",
    "    for f_idx, path in enumerate(image_paths):\n",
    "        faces = frame_faces_all[f_idx]\n",
    "        results[path][\"faces\"] = []\n",
    "        if not faces:\n",
    "            continue\n",
    "        try:\n",
    "            pil_image = Image.open(path).convert(\"RGB\")\n",
    "        except:\n",
    "            continue\n",
    "        frame_norm_boxes = []\n",
    "        frame_names = []\n",
    "        frame_emotions = []\n",
    "        frame_inout = []\n",
    "        for face in faces:\n",
    "            x1, y1, x2, y2 = face[\"bbox\"]\n",
    "            frame_norm_box = (x1/width, y1/height, x2/width, y2/height)\n",
    "            frame_norm_boxes.append(frame_norm_box)\n",
    "            frame_names.append(face[\"name\"])\n",
    "            frame_emotions.append(face[\"emotion\"])\n",
    "            complete_face = {\n",
    "                \"bbox\": face[\"bbox\"],\n",
    "                \"profile_id\": face[\"profile_id\"],\n",
    "                \"name\": face[\"name\"],\n",
    "                \"similarity_score\": face.get(\"similarity_score\"),\n",
    "                \"emotion\": face[\"emotion\"],\n",
    "                \"gaze\": {}\n",
    "            }\n",
    "            if gaze_out:\n",
    "                face_idx = faces.index(face)\n",
    "                if \"inout\" in gaze_out and f_idx < len(gaze_out[\"inout\"]) and face_idx < len(gaze_out[\"inout\"][f_idx]):\n",
    "                    inout_score = gaze_out[\"inout\"][f_idx][face_idx].item()\n",
    "                    frame_inout.append(inout_score)\n",
    "                    complete_face[\"gaze\"][\"inout_score\"] = inout_score\n",
    "                    complete_face[\"gaze\"][\"looking_at_camera\"] = inout_score > 0.5\n",
    "                else:\n",
    "                    frame_inout.append(None)\n",
    "                if \"heatmap\" in gaze_out and f_idx < len(gaze_out[\"heatmap\"]) and face_idx < len(gaze_out[\"heatmap\"][f_idx]):\n",
    "                    hm = gaze_out[\"heatmap\"][f_idx][face_idx].cpu().numpy()\n",
    "                    complete_face[\"gaze\"][\"heatmap\"] = hm.tolist()\n",
    "            results[path][\"faces\"].append(complete_face)\n",
    "        if gaze_out and \"heatmap\" in gaze_out and f_idx < len(gaze_out[\"heatmap\"]):\n",
    "            frame_hm = gaze_out[\"heatmap\"][f_idx]\n",
    "            vis_img = visualize_all(\n",
    "                pil_image,\n",
    "                frame_hm,\n",
    "                frame_norm_boxes,\n",
    "                frame_inout,\n",
    "                frame_emotions,\n",
    "                frame_names\n",
    "            )\n",
    "            visuals[path] = vis_img\n",
    "            if output_dir:\n",
    "                os.makedirs(output_dir, exist_ok=True)\n",
    "                out_path = os.path.join(output_dir, f\"viz_{os.path.basename(path)}\")\n",
    "                vis_img.save(out_path)\n",
    "                print(f\"Saved visualization for {path} to {out_path}\")\n",
    "    \n",
    "    # --- Partition profiles into multi-frame and single-frame detections ---\n",
    "    multi_profiles = {pid: prof for pid, prof in profiles.items() if len(set(prof[\"frames_seen\"])) > 1}\n",
    "    single_profiles = {pid: prof for pid, prof in profiles.items() if len(set(prof[\"frames_seen\"])) == 1}\n",
    "    \n",
    "    # --- Recluster multi-frame profiles using DBSCAN ---\n",
    "    if multi_profiles:\n",
    "        new_multi_profiles, cluster_map_multi = recluster_profiles(multi_profiles, eps=0.2)\n",
    "    else:\n",
    "        new_multi_profiles, cluster_map_multi = {}, {}\n",
    "    \n",
    "    # For single-frame detections, assign new unique IDs starting after multi clusters.\n",
    "    offset = len(new_multi_profiles)\n",
    "    new_single_profiles = {}\n",
    "    for i, (pid, prof) in enumerate(sorted(single_profiles.items())):\n",
    "        new_id = offset + i + 1\n",
    "        new_single_profiles[new_id] = {\"name\": prof[\"name\"], \"frames_seen\": prof[\"frames_seen\"]}\n",
    "    \n",
    "    # Build a unified mapping: for multi, map old pid -> new id, for single use sorted order.\n",
    "    all_mapping = {}\n",
    "    for old_pid in multi_profiles:\n",
    "        all_mapping[old_pid] = cluster_map_multi[old_pid] + 1\n",
    "    sorted_single = sorted(single_profiles.items(), key=lambda x: x[0])\n",
    "    for i, (old_pid, prof) in enumerate(sorted_single):\n",
    "        new_id = offset + i + 1\n",
    "        all_mapping[old_pid] = new_id\n",
    "    \n",
    "    # Update each face in results with new profile id from all_mapping.\n",
    "    for frame_path, data in results.items():\n",
    "        if frame_path in ['profiles', 'visualizations']:\n",
    "            continue\n",
    "        for face in data.get(\"faces\", []):\n",
    "            old_pid = face[\"profile_id\"]\n",
    "            if old_pid in all_mapping:\n",
    "                face[\"profile_id\"] = all_mapping[old_pid]\n",
    "    \n",
    "    # Build the final profiles dictionary.\n",
    "    combined_profiles = {}\n",
    "    for label, p in new_multi_profiles.items():\n",
    "        combined_profiles[label + 1] = {\"name\": p[\"name\"], \"frames_seen\": sorted(list(set(p[\"frames_seen\"])))}\n",
    "    for new_id, p in new_single_profiles.items():\n",
    "        combined_profiles[new_id] = {\"name\": p[\"name\"], \"frames_seen\": sorted(list(set(p[\"frames_seen\"])))}\n",
    "    results[\"profiles\"] = combined_profiles\n",
    "    results[\"visualizations\"] = visuals\n",
    "    print(\"Analysis complete!\")\n",
    "    llm_data = create_llm_input(results, llm_output_path)\n",
    "    return results, llm_data\n",
    "\n",
    "# -----------------------------\n",
    "# Display Analysis Results\n",
    "# -----------------------------\n",
    "def display_results(results):\n",
    "    visual_out = results.get(\"visualizations\", {})\n",
    "    for path, vis_img in visual_out.items():\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        plt.imshow(vis_img)\n",
    "        plt.title(f\"Analysis for {os.path.basename(path)}\")\n",
    "        plt.axis(\"off\")\n",
    "        plt.show()\n",
    "    profs = results.get(\"profiles\", {})\n",
    "    if profs:\n",
    "        print(\"\\nProfile Summary:\")\n",
    "        for pid, prof in profs.items():\n",
    "            print(f\"Profile {pid} ({prof['name']}): Appeared in {len(prof['frames_seen'])} frames\")\n",
    "\n",
    "# -----------------------------\n",
    "# Main Execution Block\n",
    "# -----------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    image_paths = [\n",
    "        \"frame_pbm_0000.png\", \"frame_pbm_0001.png\", \"frame_pbm_0002.png\", \"frame_pbm_0003.png\", \"frame_pbm_0004.png\",\n",
    "        \"frame_pbm_0005.png\", \"frame_pbm_0006.png\", \"frame_pbm_0007.png\", \"frame_pbm_0008.png\", \"frame_pbm_0009.png\",\n",
    "        \"frame_pbm_0010.png\", \"frame_pbm_0011.png\", \"frame_pbm_0012.png\", \"frame_pbm_0013.png\", \"frame_pbm_0014.png\",\n",
    "        \"frame_pbm_0015.png\", \"frame_pbm_0016.png\", \"frame_pbm_0017.png\", \"frame_pbm_0018.png\", \"frame_pbm_0019.png\",\n",
    "    ]\n",
    "    # Load gaze detection model\n",
    "    gaze_model, _ = torch.hub.load('fkryan/gazelle', 'gazelle_dinov2_vitl14_inout')\n",
    "    gaze_model.eval()\n",
    "    gaze_model.to('cpu')\n",
    "    # Load YOLO face detection model (replace with your YOLO–face model if available)\n",
    "    yolo_model_path = \"yolov8n.pt\"\n",
    "    print(f\"Loading YOLO model from {yolo_model_path} on {device}...\")\n",
    "    try:\n",
    "        yolo_face_model = YOLO(yolo_model_path)\n",
    "        yolo_face_model.to(device)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading YOLO model: {e}\")\n",
    "        exit(1)\n",
    "    # For this run, we set known_faces to None (thus no known faces are used)\n",
    "    known_faces = None\n",
    "    # Analyze frames\n",
    "    results, llm_data = analyze_frames(\n",
    "        image_paths,\n",
    "        gaze_model,\n",
    "        yolo_face_model,\n",
    "        known_faces,\n",
    "        output_dir=\"output\",\n",
    "        llm_output_path=\"llm_analysis_input.json\"\n",
    "    )\n",
    "    # Display visual results\n",
    "    display_results(results)\n",
    "    print(\"\\nLLM data has been created and saved to llm_analysis_input.json\")\n",
    "    print(\"This data can now be sent to an LLM for analysis.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "office2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
