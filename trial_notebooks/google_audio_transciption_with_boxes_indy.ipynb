{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "import subprocess\n",
    "import cv2\n",
    "import yt_dlp\n",
    "from ffpyplayer.player import MediaPlayer\n",
    "from google.cloud import speech\n",
    "from queue import Queue\n",
    "import threading\n",
    "import time\n",
    "import mediapipe as mp\n",
    "from mediapipe.tasks import python\n",
    "from mediapipe.tasks.python import vision\n",
    "\n",
    "# Set Google Cloud credentials\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"/Users/indrajababu/Downloads/seismic-rarity-427422-p7-ab3b4a8726ef.json\"\n",
    "\n",
    "# Global variables for face detection and speaker tracking\n",
    "detection_results = []  # Stores face detection results\n",
    "current_speaker_tag = None  # Tracks the current speaker\n",
    "speaker_faces = {}  # Maps speaker tags to face positions\n",
    "face_detector = None  # Will hold our face detector instance\n",
    "\n",
    "def initialize_face_detector():\n",
    "    \"\"\"Initialize the MediaPipe face detector with proper error handling.\"\"\"\n",
    "    global face_detector\n",
    "    try:\n",
    "        model_path = \"detector.tflite\"  # Update this path as needed\n",
    "        \n",
    "        # Check if model file exists\n",
    "        if not os.path.exists(model_path):\n",
    "            print(f\"Error: Model file not found at {model_path}\")\n",
    "            return None\n",
    "\n",
    "        BaseOptions = mp.tasks.BaseOptions\n",
    "        FaceDetector = mp.tasks.vision.FaceDetector\n",
    "        FaceDetectorOptions = mp.tasks.vision.FaceDetectorOptions\n",
    "        VisionRunningMode = mp.tasks.vision.RunningMode\n",
    "\n",
    "        options = FaceDetectorOptions(\n",
    "            base_options=BaseOptions(model_asset_path=model_path),\n",
    "            running_mode=VisionRunningMode.LIVE_STREAM,\n",
    "            result_callback=print_result\n",
    "        )\n",
    "\n",
    "        face_detector = vision.FaceDetector.create_from_options(options)\n",
    "        return face_detector\n",
    "    except Exception as e:\n",
    "        print(f\"Error initializing face detector: {e}\")\n",
    "        return None\n",
    "\n",
    "def print_result(result, output_image, timestamp_ms):\n",
    "    \"\"\"Callback function for face detection results.\"\"\"\n",
    "    global detection_results\n",
    "    try:\n",
    "        detection_results = result.detections if result.detections else []\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing face detection results: {e}\")\n",
    "        detection_results = []\n",
    "\n",
    "def visualize(image, detections, speaker_tag=None):\n",
    "    \"\"\"Draw bounding boxes on the image with special highlighting for current speaker.\"\"\"\n",
    "    try:\n",
    "        annotated_image = image.copy()\n",
    "        height, width, _ = image.shape\n",
    "        \n",
    "        for detection in detections:\n",
    "            bbox = detection.bounding_box\n",
    "            start_point = (bbox.origin_x, bbox.origin_y)\n",
    "            end_point = (bbox.origin_x + bbox.width, bbox.origin_y + bbox.height)\n",
    "            \n",
    "            # Default color (green) for non-speaking faces\n",
    "            color = (0, 255, 0)\n",
    "            thickness = 2\n",
    "            \n",
    "            # If we have speaker information and want to highlight current speaker\n",
    "            if speaker_tag is not None:\n",
    "                # Store face position for this speaker (simple approach - just use the first face)\n",
    "                speaker_faces[speaker_tag] = (start_point, end_point)\n",
    "                \n",
    "                # If this is the current speaker, change color to red\n",
    "                if speaker_tag == current_speaker_tag:\n",
    "                    color = (0, 0, 255)\n",
    "                    thickness = 3\n",
    "            \n",
    "            # Draw the bounding box\n",
    "            cv2.rectangle(annotated_image, start_point, end_point, color, thickness)\n",
    "        \n",
    "        return annotated_image\n",
    "    except Exception as e:\n",
    "        print(f\"Error in visualization: {e}\")\n",
    "        return image\n",
    "\n",
    "def transcribe_audio_stream(audio_url, transcription_queue):\n",
    "    \"\"\"Stream audio for transcription using Google Cloud Speech-to-Text.\"\"\"\n",
    "    global current_speaker_tag\n",
    "    \n",
    "    client = speech.SpeechClient()\n",
    "\n",
    "    # Use ffmpeg to convert audio stream to raw PCM data\n",
    "    ffmpeg_command = [\n",
    "        \"ffmpeg\", \"-i\", audio_url, \"-f\", \"s16le\", \"-ac\", \"1\", \"-ar\", \"16000\",\n",
    "        \"-loglevel\", \"quiet\", \"pipe:1\"\n",
    "    ]\n",
    "    \n",
    "    diarization_config = speech.SpeakerDiarizationConfig(\n",
    "        enable_speaker_diarization=True,\n",
    "        min_speaker_count=2,\n",
    "        max_speaker_count=10,\n",
    "    )\n",
    "    process = subprocess.Popen(ffmpeg_command, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "\n",
    "    streaming_config = speech.RecognitionConfig(\n",
    "        encoding=speech.RecognitionConfig.AudioEncoding.LINEAR16,\n",
    "        sample_rate_hertz=16000,\n",
    "        language_code=\"en-US\",\n",
    "        diarization_config=diarization_config\n",
    "    )\n",
    "    streaming_request = speech.StreamingRecognitionConfig(config=streaming_config, interim_results=True)\n",
    "\n",
    "    def audio_generator():\n",
    "        while True:\n",
    "            data = process.stdout.read(4096)\n",
    "            if not data:\n",
    "                break\n",
    "            yield data\n",
    "\n",
    "    requests = (speech.StreamingRecognizeRequest(audio_content=chunk) for chunk in audio_generator())\n",
    "    \n",
    "    try:\n",
    "        responses = client.streaming_recognize(config=streaming_request, requests=requests)\n",
    "        \n",
    "        for response in responses:\n",
    "            if not response.results:\n",
    "                continue\n",
    "                \n",
    "            result = response.results[-1]  # Get the latest result\n",
    "            \n",
    "            if not result.alternatives:\n",
    "                continue\n",
    "                \n",
    "            # Only process if we have word-level info with speaker tags\n",
    "            if hasattr(result.alternatives[0], 'words') and result.alternatives[0].words:\n",
    "                local_speaker_tag = None\n",
    "                final_transcription = [[]]\n",
    "                \n",
    "                words_info = result.alternatives[0].words\n",
    "                \n",
    "                # Process each word and track speaker change\n",
    "                for word_info in words_info:\n",
    "                    # If speaker changes, append the previous speaker's transcription\n",
    "                    if local_speaker_tag is None or word_info.speaker_tag != local_speaker_tag:\n",
    "                        if local_speaker_tag is not None:\n",
    "                            transcription = \" \".join(final_transcription[-1])\n",
    "                            timestamp = time.time()\n",
    "                            transcription_queue.put((timestamp, f\"Speaker {local_speaker_tag}: {transcription}\"))\n",
    "                            current_speaker_tag = local_speaker_tag  # Update global speaker tag\n",
    "                        \n",
    "                        final_transcription.append([])\n",
    "                        local_speaker_tag = word_info.speaker_tag\n",
    "                    \n",
    "                    final_transcription[-1].append(word_info.word)\n",
    "                \n",
    "                # Add the last speaker's transcription\n",
    "                if local_speaker_tag is not None and final_transcription[-1]:\n",
    "                    transcription = \" \".join(final_transcription[-1])\n",
    "                    timestamp = time.time()\n",
    "                    transcription_queue.put((timestamp, f\"Speaker {local_speaker_tag}: {transcription}\"))\n",
    "                    current_speaker_tag = local_speaker_tag  # Update global speaker tag\n",
    "                    \n",
    "            elif result.is_final and result.alternatives[0].transcript:\n",
    "                # Fallback for when speaker diarization isn't available\n",
    "                timestamp = time.time()\n",
    "                transcription_queue.put((timestamp, result.alternatives[0].transcript))\n",
    "\n",
    "    except Exception as e:\n",
    "        exc_type, exc_obj, exc_tb = sys.exc_info()\n",
    "        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]\n",
    "        print(f\"Error in transcription: {exc_type}, {fname}, line {exc_tb.tb_lineno}\")\n",
    "        print(f\"Exception details: {e}\")\n",
    "    finally:\n",
    "        process.terminate()\n",
    "\n",
    "def play_video_with_audio_and_transcription(video_url):\n",
    "    \"\"\"Play video with synchronized audio and perform real-time transcription.\"\"\"\n",
    "    global detection_results, current_speaker_tag, face_detector\n",
    "    \n",
    "    try:\n",
    "        # First, list available formats to debug\n",
    "        print(\"Listing available formats:\")\n",
    "        with yt_dlp.YoutubeDL({\"listformats\": True}) as ydl:\n",
    "            ydl.extract_info(video_url, download=False)\n",
    "        \n",
    "        # yt-dlp options with more flexible format selection\n",
    "        ydl_opts = {\n",
    "            \"format\": \"bestvideo[height<=720]+bestaudio/best[height<=720]\",\n",
    "            \"quiet\": True,\n",
    "            \"no_warnings\": True\n",
    "        }\n",
    "\n",
    "        # Fetch video info\n",
    "        with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
    "            info = ydl.extract_info(video_url, download=False)\n",
    "            \n",
    "            # Handle different info structures\n",
    "            if 'url' in info:\n",
    "                video_stream_url = info['url']\n",
    "            elif 'requested_formats' in info:\n",
    "                for fmt in info['requested_formats']:\n",
    "                    if fmt.get('vcodec', 'none') != 'none':\n",
    "                        video_stream_url = fmt['url']\n",
    "                        break\n",
    "                else:\n",
    "                    raise KeyError(\"No video URL found in requested formats\")\n",
    "            else:\n",
    "                raise KeyError(\"Could not find URL in the extracted information\")\n",
    "\n",
    "        # Fetch audio URL with similar approach\n",
    "        audio_opts = {\n",
    "            \"format\": \"bestaudio/best\",\n",
    "            \"quiet\": True,\n",
    "            \"no_warnings\": True\n",
    "        }\n",
    "        \n",
    "        with yt_dlp.YoutubeDL(audio_opts) as ydl:\n",
    "            audio_info = ydl.extract_info(video_url, download=False)\n",
    "            \n",
    "            if 'url' in audio_info:\n",
    "                audio_stream_url = audio_info['url']\n",
    "            elif 'requested_formats' in audio_info:\n",
    "                for fmt in audio_info['requested_formats']:\n",
    "                    if fmt.get('acodec', 'none') != 'none':\n",
    "                        audio_stream_url = fmt['url']\n",
    "                        break\n",
    "                else:\n",
    "                    audio_stream_url = video_stream_url\n",
    "            else:\n",
    "                audio_stream_url = video_stream_url\n",
    "\n",
    "        print(f\"Video URL: {video_stream_url[:50]}...\")\n",
    "        print(f\"Audio URL: {audio_stream_url[:50]}...\")\n",
    "\n",
    "        # Initialize OpenCV video capture\n",
    "        cap = cv2.VideoCapture(video_stream_url)\n",
    "        if not cap.isOpened():\n",
    "            print(\"Error: Cannot open video stream.\")\n",
    "            return\n",
    "\n",
    "        fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "        if fps <= 0:\n",
    "            fps = 30\n",
    "        frame_delay = int(1000 / fps)\n",
    "\n",
    "        # Initialize ffpyplayer for audio\n",
    "        player = MediaPlayer(video_stream_url)\n",
    "\n",
    "        # Queue for synchronized transcription\n",
    "        transcription_queue = Queue()\n",
    "\n",
    "        # Start transcription in a background thread\n",
    "        transcription_thread = threading.Thread(\n",
    "            target=transcribe_audio_stream, args=(audio_stream_url, transcription_queue)\n",
    "        )\n",
    "        transcription_thread.daemon = True\n",
    "        transcription_thread.start()\n",
    "\n",
    "        # Initialize face detector\n",
    "        face_detector = initialize_face_detector()\n",
    "        if face_detector is None:\n",
    "            print(\"Warning: Face detection disabled due to initialization error\")\n",
    "\n",
    "        print(\"Press 'q' to quit the video stream.\")\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                print(\"End of stream or cannot fetch frame.\")\n",
    "                break\n",
    "\n",
    "            # Convert frame to RGB if needed (MediaPipe expects RGB)\n",
    "            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            \n",
    "            # Detect faces if detector is available\n",
    "            if face_detector is not None:\n",
    "                try:\n",
    "                    mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=frame_rgb)\n",
    "                    face_detector.detect_async(mp_image, int(time.time() * 1000))\n",
    "                except Exception as e:\n",
    "                    print(f\"Error in face detection: {e}\")\n",
    "\n",
    "            # Visualize faces with speaker highlighting\n",
    "            try:\n",
    "                annotated_frame = visualize(frame, detection_results, current_speaker_tag)\n",
    "            except Exception as e:\n",
    "                print(f\"Error in visualization: {e}\")\n",
    "                annotated_frame = frame\n",
    "            \n",
    "            # Display video frame\n",
    "            cv2.imshow('YouTube Video Stream', annotated_frame)\n",
    "\n",
    "            # Play audio synchronously\n",
    "            audio_frame, val = player.get_frame()\n",
    "            if val != 'eof' and audio_frame:\n",
    "                # Check for and display transcriptions\n",
    "                while not transcription_queue.empty():\n",
    "                    transcription_time, transcription = transcription_queue.queue[0]\n",
    "                    if transcription_time <= time.time():\n",
    "                        print(f\"Transcript: {transcription}\")\n",
    "                        transcription_queue.get()\n",
    "                    else:\n",
    "                        break\n",
    "\n",
    "            # Exit on pressing 'q'\n",
    "            if cv2.waitKey(frame_delay) & 0xFF == ord('q'):\n",
    "                break\n",
    "\n",
    "        # Release resources\n",
    "        cap.release()\n",
    "        player.close_player()\n",
    "        cv2.destroyAllWindows()\n",
    "        if face_detector is not None:\n",
    "            face_detector.close()\n",
    "\n",
    "    except Exception as e:\n",
    "        exc_type, exc_obj, exc_tb = sys.exc_info()\n",
    "        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]\n",
    "        print(f\"Error in video playback: {exc_type}, {fname}, line {exc_tb.tb_lineno}\")\n",
    "        print(f\"Exception details: {e}\")\n",
    "\n",
    "def terminate_script():\n",
    "    \"\"\"Terminates the current Python process.\"\"\"\n",
    "    os._exit(0)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Replace with your YouTube video URL\n",
    "    youtube_url = \"https://www.youtube.com/watch?v=96Y6mc3C1Bg\"  # Example video\n",
    "    \n",
    "    try:\n",
    "        play_video_with_audio_and_transcription(youtube_url)\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nScript terminated by user.\")\n",
    "        terminate_script()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Install dependencies\n",
    "conda install -c conda-forge wget opencv pytorch torchvision cudatoolkit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
