{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-24 00:49:56.280567: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video URL: https://rr1---sn-n4v7snls.googlevideo.com/videoplayback?expire=1737730217&ei=SFSTZ4PTO--KsfIP1tWBsAk&ip=2601%3A644%3A601%3Afa0%3Afdcd%3A289b%3A9a7d%3A5a7c&id=o-ADqQIUZK-DcFAw-osEzKKdKPfuAxdWTg2DAINd7omDMn&itag=18&source=youtube&requiressl=yes&xpc=EgVo2aDSNQ%3D%3D&met=1737708617%2C&mh=KN&mm=31%2C29&mn=sn-n4v7snls%2Csn-o097znzr&ms=au%2Crdu&mv=m&mvi=1&pl=48&rms=au%2Cau&initcwndbps=4846250&bui=AY2Et-Nu1dZ8JJs_WbU-SgmGDop10U0u8FSW_cl8v8BUMKJEqpq1jpc53eqIootsj63fKdt85MQXhDky&spc=9kzgDYjkdeuw-K8FCjhuj6V3Zeg3Y8Czg1WaugGspUUgvWBY7eEfRrDxSERscBwJ7w&vprv=1&svpuc=1&mime=video%2Fmp4&ns=Me9V-kZjS8m4bzyDkKC7XyAQ&rqh=1&cnr=14&ratebypass=yes&dur=5104.187&lmt=1737219722122408&mt=1737708285&fvip=5&fexp=51326932%2C51331020%2C51335594%2C51353498%2C51371294%2C51384461&c=MWEB&sefc=1&txp=5538534&n=FQeY18xEzA30sw&sparams=expire%2Cei%2Cip%2Cid%2Citag%2Csource%2Crequiressl%2Cxpc%2Cbui%2Cspc%2Cvprv%2Csvpuc%2Cmime%2Cns%2Crqh%2Ccnr%2Cratebypass%2Cdur%2Clmt&sig=AJfQdSswRgIhANApxxnuxM7EFqX_vI1cY-1hiNMhhCt8cRzH9qf5S3eUAiEAvyyQWWB64LZuW_clT3kVZGFPRECcpWdXDn5vk_S5XmE%3D&lsparams=met%2Cmh%2Cmm%2Cmn%2Cms%2Cmv%2Cmvi%2Cpl%2Crms%2Cinitcwndbps&lsig=AGluJ3MwRQIgWIzyhx-ioRcTV83XkqQwZU0Bus9MFONULUoQVeTDVT4CIQCGltI5-k5Qmoahf-7fhdaGwOJfzHQN2G83ijjK1mFUiA%3D%3D\n",
      "Loaded embedding for girl\n",
      "Loaded embedding for guy\n",
      "Processing YouTube video. Press 'q' to quit.\n",
      "Exiting video processing.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from deepface import DeepFace\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import cosine\n",
    "import yt_dlp\n",
    "import time\n",
    "\n",
    "# Configuration constants\n",
    "CASCADE_PATH = cv2.data.haarcascades + 'haarcascade_frontalface_default.xml'\n",
    "SCALE_FACTOR = 1.1\n",
    "MIN_NEIGHBORS = 5\n",
    "MIN_FACE_SIZE = (30, 30)\n",
    "FRAME_SKIP = 20  # Perform analysis every 20 frames\n",
    "PERSISTENCE_TIME = 15  # Number of skipped frames to persist bounding boxes\n",
    "KNOWN_FACES = {\n",
    "    \"girl\": \"girl_in_yt.jpg\",\n",
    "    \"guy\": \"guy_in_yt.jpg\"\n",
    "}\n",
    "\n",
    "# Global profiles to store emotions for each known face\n",
    "profiles = {name: {\"emotions\": []} for name in KNOWN_FACES.keys()}\n",
    "active_faces = {}  # Dictionary to store bounding boxes and counters for detected faces\n",
    "\n",
    "# Load and store profiles\n",
    "def load_known_faces(image_paths):\n",
    "    \"\"\"Generate embeddings for provided images.\"\"\"\n",
    "    known_faces = {}\n",
    "    for name, image_path in image_paths.items():\n",
    "        try:\n",
    "            # Load the image\n",
    "            face_image = cv2.imread(image_path)\n",
    "            if face_image is None:\n",
    "                print(f\"Error: Could not read image {image_path}\")\n",
    "                continue\n",
    "\n",
    "            # Generate embedding\n",
    "            embedding = DeepFace.represent(face_image, model_name='Facenet512', enforce_detection=False)\n",
    "            known_faces[name] = np.array(embedding[0]['embedding'])\n",
    "            print(f\"Loaded embedding for {name}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {name}: {e}\")\n",
    "    return known_faces\n",
    "\n",
    "# Classify faces uniquely to known names\n",
    "def classify_faces_uniquely(face_embeddings, known_faces):\n",
    "    \"\"\"Classify multiple detected faces uniquely to the known names.\"\"\"\n",
    "    matches = []  # To store matched names and distances\n",
    "    used_names = set()  # Keep track of assigned names\n",
    "\n",
    "    for embedding in face_embeddings:\n",
    "        best_match = None\n",
    "        best_distance = float('inf')\n",
    "\n",
    "        # Find the closest match for the embedding among remaining names\n",
    "        for name, known_embedding in known_faces.items():\n",
    "            if name in used_names:\n",
    "                continue  # Skip already assigned names\n",
    "            distance = cosine(known_embedding, embedding)\n",
    "            if distance < best_distance:\n",
    "                best_match = name\n",
    "                best_distance = distance\n",
    "\n",
    "        if best_match:\n",
    "            matches.append((best_match, best_distance))\n",
    "            used_names.add(best_match)  # Mark the name as used\n",
    "\n",
    "    return matches\n",
    "\n",
    "# Detect faces in a frame\n",
    "def detect_faces(face_cascade, frame):\n",
    "    \"\"\"Detect faces in the given frame.\"\"\"\n",
    "    gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_cascade.detectMultiScale(\n",
    "        gray_frame, \n",
    "        scaleFactor=SCALE_FACTOR, \n",
    "        minNeighbors=MIN_NEIGHBORS, \n",
    "        minSize=MIN_FACE_SIZE\n",
    "    )\n",
    "    return faces\n",
    "\n",
    "# Generate face embedding\n",
    "def get_face_embedding(face_roi):\n",
    "    \"\"\"Generate an embedding for the given face ROI.\"\"\"\n",
    "    try:\n",
    "        embedding = DeepFace.represent(face_roi, model_name='Facenet512', enforce_detection=False)\n",
    "        return np.array(embedding[0]['embedding'])\n",
    "    except Exception as e:\n",
    "        print(f\"Embedding error: {e}\")\n",
    "        return None\n",
    "\n",
    "# Analyze emotions in the face ROI\n",
    "def analyze_emotions(face_roi):\n",
    "    \"\"\"Analyze emotions using DeepFace and return all emotion scores.\"\"\"\n",
    "    try:\n",
    "        result = DeepFace.analyze(face_roi, actions=['emotion'], enforce_detection=False)\n",
    "        return result[0]['emotion']  # Return the entire emotion dictionary\n",
    "    except Exception as e:\n",
    "        print(f\"Emotion analysis error: {e}\")\n",
    "        return {}\n",
    "\n",
    "def get_video_url(youtube_url):\n",
    "    \"\"\"Fetch the direct video URL using yt-dlp.\"\"\"\n",
    "    ydl_opts = {\n",
    "        'quiet': True,\n",
    "        'format': 'best[ext=mp4]'\n",
    "    }\n",
    "    with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
    "        info = ydl.extract_info(youtube_url, download=False)\n",
    "        return info['url']\n",
    "\n",
    "# Main script\n",
    "def main():\n",
    "    # YouTube video URL\n",
    "    youtube_url = \"https://www.youtube.com/watch?v=96Y6mc3C1Bg\"  # Replace with your video URL\n",
    "    video_url = get_video_url(youtube_url)\n",
    "    print(\"Video URL:\", video_url)\n",
    "\n",
    "    # Open the YouTube video stream\n",
    "    cap = cv2.VideoCapture(video_url)\n",
    "    if not cap.isOpened():\n",
    "        raise RuntimeError(\"Error: Could not open the YouTube video stream.\")\n",
    "\n",
    "    # Load known face embeddings\n",
    "    known_faces = load_known_faces(KNOWN_FACES)\n",
    "\n",
    "    # Load face cascade classifier\n",
    "    face_cascade = cv2.CascadeClassifier(CASCADE_PATH)\n",
    "\n",
    "    frame_counter = 0  # Counter to keep track of skipped frames\n",
    "\n",
    "    try:\n",
    "        print(\"Processing YouTube video. Press 'q' to quit.\")\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                print(\"End of video or error reading the video stream.\")\n",
    "                break\n",
    "\n",
    "            # Increment frame counter\n",
    "            frame_counter += 1\n",
    "\n",
    "            if frame_counter % FRAME_SKIP == 0:\n",
    "                # Perform face detection and analysis every `FRAME_SKIP` frames\n",
    "                faces = detect_faces(face_cascade, frame)\n",
    "\n",
    "                face_embeddings = []  # List to store embeddings for all detected faces\n",
    "                bounding_boxes = []  # List to store bounding boxes for all detected faces\n",
    "\n",
    "                for (x, y, w, h) in faces:\n",
    "                    # Extract the face ROI\n",
    "                    face_roi = frame[y:y + h, x:x + w]\n",
    "                    if face_roi.size == 0:\n",
    "                        continue\n",
    "\n",
    "                    # Get face embedding\n",
    "                    embedding = get_face_embedding(face_roi)\n",
    "                    if embedding is None:\n",
    "                        continue\n",
    "\n",
    "                    face_embeddings.append(embedding)\n",
    "                    bounding_boxes.append((x, y, w, h))\n",
    "\n",
    "                # Classify faces uniquely to known names\n",
    "                matches = classify_faces_uniquely(face_embeddings, known_faces)\n",
    "\n",
    "                for i, (name, distance) in enumerate(matches):\n",
    "                    x, y, w, h = bounding_boxes[i]\n",
    "\n",
    "                    # Analyze emotions\n",
    "                    emotions = analyze_emotions(frame[y:y + h, x:x + w])\n",
    "\n",
    "                    # Update profile with the detected emotions\n",
    "                    if name in profiles:\n",
    "                        profiles[name][\"emotions\"].append(emotions)\n",
    "\n",
    "                    # Store the detected face and reset its persistence counter\n",
    "                    active_faces[name] = {\n",
    "                        \"box\": (x, y, w, h),\n",
    "                        \"emotions\": emotions,\n",
    "                        \"counter\": PERSISTENCE_TIME  # Reset the persistence counter\n",
    "                    }\n",
    "\n",
    "            # Decrement counters for inactive faces\n",
    "            for name in list(active_faces.keys()):\n",
    "                active_faces[name][\"counter\"] -= 1\n",
    "                if active_faces[name][\"counter\"] <= 0:\n",
    "                    del active_faces[name]  # Remove expired bounding boxes\n",
    "\n",
    "            # Draw all active bounding boxes\n",
    "            for name, data in active_faces.items():\n",
    "                x, y, w, h = data[\"box\"]\n",
    "                emotions = data[\"emotions\"]\n",
    "                color = (0, 255, 0)  # Green for bounding box\n",
    "                cv2.rectangle(frame, (x, y), (x + w, y + h), color, 2)\n",
    "\n",
    "                # Display the name and emotion scores\n",
    "                label = f\"{name}\"  # Add the name to the label\n",
    "                y_offset = y - 10\n",
    "                cv2.putText(frame, label, (x, y_offset), cv2.FONT_HERSHEY_SIMPLEX, 0.7, color, 2)\n",
    "                y_offset -= 20\n",
    "                for emotion, score in emotions.items():\n",
    "                    cv2.putText(frame, f\"{emotion}: {score:.2f}%\", (x, y_offset), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 1)\n",
    "                    y_offset -= 15\n",
    "\n",
    "            # Display the resulting frame\n",
    "            cv2.imshow('Face Recognition with Emotion Scores', frame)\n",
    "\n",
    "            # Adjustable frame delay to match playback speed\n",
    "            time.sleep(0.04)  # Adjust this to sync the video with the actual playback rate\n",
    "\n",
    "            # Press 'q' to exit\n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                print(\"Exiting video processing.\")\n",
    "                break\n",
    "    finally:\n",
    "        cap.release()\n",
    "        cv2.destroyAllWindows()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-24 01:31:18.311165: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name '__version__' from 'retinaface' (unknown location)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mcv2\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdeepface\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DeepFace\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mspatial\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistance\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m cosine\n",
      "File \u001b[0;32m/opt/miniconda3/envs/office/lib/python3.9/site-packages/deepface/DeepFace.py:20\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdeepface\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcommons\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m package_utils, folder_utils\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdeepface\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcommons\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlogger\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Logger\n\u001b[0;32m---> 20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdeepface\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodules\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     21\u001b[0m     modeling,\n\u001b[1;32m     22\u001b[0m     representation,\n\u001b[1;32m     23\u001b[0m     verification,\n\u001b[1;32m     24\u001b[0m     recognition,\n\u001b[1;32m     25\u001b[0m     demography,\n\u001b[1;32m     26\u001b[0m     detection,\n\u001b[1;32m     27\u001b[0m     streaming,\n\u001b[1;32m     28\u001b[0m     preprocessing,\n\u001b[1;32m     29\u001b[0m )\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdeepface\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m __version__\n\u001b[1;32m     32\u001b[0m logger \u001b[38;5;241m=\u001b[39m Logger()\n",
      "File \u001b[0;32m/opt/miniconda3/envs/office/lib/python3.9/site-packages/deepface/modules/modeling.py:16\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# project dependencies\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdeepface\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfacial_recognition\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      6\u001b[0m     VGGFace,\n\u001b[1;32m      7\u001b[0m     OpenFace,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     14\u001b[0m     GhostFaceNet,\n\u001b[1;32m     15\u001b[0m )\n\u001b[0;32m---> 16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdeepface\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mface_detection\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     17\u001b[0m     FastMtCnn,\n\u001b[1;32m     18\u001b[0m     MediaPipe,\n\u001b[1;32m     19\u001b[0m     MtCnn,\n\u001b[1;32m     20\u001b[0m     OpenCv,\n\u001b[1;32m     21\u001b[0m     Dlib \u001b[38;5;28;01mas\u001b[39;00m DlibDetector,\n\u001b[1;32m     22\u001b[0m     RetinaFace,\n\u001b[1;32m     23\u001b[0m     Ssd,\n\u001b[1;32m     24\u001b[0m     Yolo,\n\u001b[1;32m     25\u001b[0m     YuNet,\n\u001b[1;32m     26\u001b[0m     CenterFace,\n\u001b[1;32m     27\u001b[0m )\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdeepface\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdemography\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Age, Gender, Race, Emotion\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdeepface\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mspoofing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FasNet\n",
      "File \u001b[0;32m/opt/miniconda3/envs/office/lib/python3.9/site-packages/deepface/models/face_detection/RetinaFace.py:3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m List\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mretinaface\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m RetinaFace \u001b[38;5;28;01mas\u001b[39;00m rf\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdeepface\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mDetector\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Detector, FacialAreaRegion\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# pylint: disable=too-few-public-methods\u001b[39;00m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/office/lib/python3.9/site-packages/retinaface/RetinaFace.py:13\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mretinaface\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m __version__\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mretinaface\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m retinaface_model\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mretinaface\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcommons\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m preprocess, postprocess\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name '__version__' from 'retinaface' (unknown location)"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from deepface import DeepFace\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import cosine\n",
    "import yt_dlp\n",
    "import time\n",
    "import torch\n",
    "import torchvision.transforms as T\n",
    "\n",
    "# Configuration constants\n",
    "CASCADE_PATH = cv2.data.haarcascades + 'haarcascade_frontalface_default.xml'\n",
    "SCALE_FACTOR = 1.1\n",
    "MIN_NEIGHBORS = 5\n",
    "MIN_FACE_SIZE = (30, 30)\n",
    "FRAME_SKIP = 20\n",
    "PERSISTENCE_TIME = 15\n",
    "KNOWN_FACES = {\n",
    "    \"girl\": \"girl_in_yt.jpg\",\n",
    "    \"guy\": \"guy_in_yt.jpg\"\n",
    "}\n",
    "\n",
    "# Initialize gaze detection model\n",
    "def initialize_gaze_model():\n",
    "    print(\"Loading Gaze-LLE model...\")\n",
    "    model, _ = torch.hub.load('fkryan/gazelle', 'gazelle_dinov2_vitb14', trust_repo=True)\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    transform = T.Compose([\n",
    "        T.Resize((448, 448)),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "    print(\"Model and transform loaded successfully.\")\n",
    "    return model, transform\n",
    "\n",
    "# Detect gaze target\n",
    "def detect_gaze(model, transform, frame, bounding_boxes):\n",
    "    \"\"\"Estimate gaze direction and return gaze points.\"\"\"\n",
    "    gaze_results = []\n",
    "    for bbox in bounding_boxes:\n",
    "        x, y, w, h = bbox\n",
    "        face_roi = frame[y:y + h, x:x + w]\n",
    "        face_pil = cv2.cvtColor(face_roi, cv2.COLOR_BGR2RGB)\n",
    "        face_pil = Image.fromarray(face_pil)\n",
    "\n",
    "        # Preprocess the face for gaze estimation\n",
    "        face_tensor = transform(face_pil).unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            output = model({\"images\": face_tensor})\n",
    "            heatmap = output['heatmap'][0].detach().cpu().numpy()\n",
    "            max_index = np.unravel_index(np.argmax(heatmap), heatmap.shape)\n",
    "            gaze_x = max_index[1] / heatmap.shape[1] * frame.shape[1]\n",
    "            gaze_y = max_index[0] / heatmap.shape[0] * frame.shape[0]\n",
    "\n",
    "        gaze_results.append((gaze_x, gaze_y))\n",
    "    return gaze_results\n",
    "\n",
    "# Check who the gaze is directed at\n",
    "def determine_gaze_target(gaze_points, bounding_boxes, names):\n",
    "    \"\"\"Determine who each person is looking at based on gaze points.\"\"\"\n",
    "    gaze_targets = []\n",
    "    for i, (gx, gy) in enumerate(gaze_points):\n",
    "        target = None\n",
    "        min_distance = float('inf')\n",
    "        for j, bbox in enumerate(bounding_boxes):\n",
    "            if i == j:\n",
    "                continue\n",
    "            x, y, w, h = bbox\n",
    "            bbox_center_x = x + w / 2\n",
    "            bbox_center_y = y + h / 2\n",
    "            distance = np.sqrt((gx - bbox_center_x) ** 2 + (gy - bbox_center_y) ** 2)\n",
    "            if distance < min_distance:\n",
    "                min_distance = distance\n",
    "                target = names[j]\n",
    "        gaze_targets.append(target)\n",
    "    return gaze_targets\n",
    "\n",
    "# Main script\n",
    "def main():\n",
    "    # Initialize models\n",
    "    model, transform = initialize_gaze_model()\n",
    "\n",
    "    # YouTube video URL\n",
    "    youtube_url = \"https://www.youtube.com/watch?v=96Y6mc3C1Bg\"\n",
    "    video_url = get_video_url(youtube_url)\n",
    "    print(\"Video URL:\", video_url)\n",
    "\n",
    "    # Open the YouTube video stream\n",
    "    cap = cv2.VideoCapture(video_url)\n",
    "    if not cap.isOpened():\n",
    "        raise RuntimeError(\"Error: Could not open the YouTube video stream.\")\n",
    "\n",
    "    # Load known face embeddings\n",
    "    known_faces = load_known_faces(KNOWN_FACES)\n",
    "\n",
    "    # Load face cascade classifier\n",
    "    face_cascade = cv2.CascadeClassifier(CASCADE_PATH)\n",
    "\n",
    "    frame_counter = 0\n",
    "\n",
    "    try:\n",
    "        print(\"Processing YouTube video. Press 'q' to quit.\")\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                print(\"End of video or error reading the video stream.\")\n",
    "                break\n",
    "\n",
    "            frame_counter += 1\n",
    "\n",
    "            if frame_counter % FRAME_SKIP == 0:\n",
    "                faces = detect_faces(face_cascade, frame)\n",
    "                face_embeddings = []\n",
    "                bounding_boxes = []\n",
    "                face_names = []\n",
    "\n",
    "                for (x, y, w, h) in faces:\n",
    "                    face_roi = frame[y:y + h, x:x + w]\n",
    "                    if face_roi.size == 0:\n",
    "                        continue\n",
    "\n",
    "                    embedding = get_face_embedding(face_roi)\n",
    "                    if embedding is None:\n",
    "                        continue\n",
    "\n",
    "                    match = classify_faces_uniquely([embedding], known_faces)\n",
    "                    if match:\n",
    "                        name, _ = match[0]\n",
    "                        face_names.append(name)\n",
    "                        bounding_boxes.append((x, y, w, h))\n",
    "                        face_embeddings.append(embedding)\n",
    "\n",
    "                # Detect gaze\n",
    "                gaze_points = detect_gaze(model, transform, frame, bounding_boxes)\n",
    "                gaze_targets = determine_gaze_target(gaze_points, bounding_boxes, face_names)\n",
    "\n",
    "                # Draw bounding boxes and gaze lines\n",
    "                for i, bbox in enumerate(bounding_boxes):\n",
    "                    x, y, w, h = bbox\n",
    "                    name = face_names[i]\n",
    "                    gaze_x, gaze_y = gaze_points[i]\n",
    "                    target = gaze_targets[i]\n",
    "\n",
    "                    color = (0, 255, 0)\n",
    "                    cv2.rectangle(frame, (x, y), (x + w, y + h), color, 2)\n",
    "                    cv2.putText(frame, f\"{name} -> {target}\", (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.7, color, 2)\n",
    "                    cv2.line(frame, (x + w // 2, y + h // 2), (int(gaze_x), int(gaze_y)), color, 2)\n",
    "\n",
    "            # Display the resulting frame\n",
    "            cv2.imshow('Face Recognition with Emotion and Gaze', frame)\n",
    "            time.sleep(0.04)\n",
    "\n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                print(\"Exiting video processing.\")\n",
    "                break\n",
    "    finally:\n",
    "        cap.release()\n",
    "        cv2.destroyAllWindows()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mget_ipython\u001b[49m()\u001b[38;5;241m.\u001b[39mrun_line_magic(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpip\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minstall deepface\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      2\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39mrun_line_magic(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpip\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minstall tensorflow\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mget_ipython\u001b[49m()\u001b[38;5;241m.\u001b[39mrun_line_magic(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpip\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minstall deepface\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      2\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39mrun_line_magic(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpip\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minstall tensorflow\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m_pydevd_bundle\\\\pydevd_cython.pyx:1697\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.SafeCallWrapper.__call__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle\\\\pydevd_cython.pyx:634\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle\\\\pydevd_cython.pyx:1112\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle\\\\pydevd_cython.pyx:1090\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle\\\\pydevd_cython.pyx:494\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.do_wait_suspend\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/office/lib/python3.9/site-packages/debugpy/_vendored/pydevd/pydevd.py:2185\u001b[0m, in \u001b[0;36mPyDB.do_wait_suspend\u001b[0;34m(self, thread, frame, event, arg, exception_type)\u001b[0m\n\u001b[1;32m   2182\u001b[0m             from_this_thread\u001b[38;5;241m.\u001b[39mappend(frame_custom_thread_id)\n\u001b[1;32m   2184\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_threads_suspended_single_notification\u001b[38;5;241m.\u001b[39mnotify_thread_suspended(thread_id, thread, stop_reason):\n\u001b[0;32m-> 2185\u001b[0m         keep_suspended \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_wait_suspend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mthread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrace_suspend_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_this_thread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframes_tracker\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2187\u001b[0m frames_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   2189\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m keep_suspended:\n\u001b[1;32m   2190\u001b[0m     \u001b[38;5;66;03m# This means that we should pause again after a set next statement.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/office/lib/python3.9/site-packages/debugpy/_vendored/pydevd/pydevd.py:2254\u001b[0m, in \u001b[0;36mPyDB._do_wait_suspend\u001b[0;34m(self, thread, frame, event, arg, trace_suspend_type, from_this_thread, frames_tracker)\u001b[0m\n\u001b[1;32m   2251\u001b[0m                 queue\u001b[38;5;241m.\u001b[39mput(internal_cmd)\n\u001b[1;32m   2252\u001b[0m                 wait_timeout \u001b[38;5;241m=\u001b[39m TIMEOUT_FAST\n\u001b[0;32m-> 2254\u001b[0m         \u001b[43mnotify_event\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwait_timeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2255\u001b[0m         notify_event\u001b[38;5;241m.\u001b[39mclear()\n\u001b[1;32m   2257\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/miniconda3/envs/office/lib/python3.9/threading.py:581\u001b[0m, in \u001b[0;36mEvent.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    579\u001b[0m signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flag\n\u001b[1;32m    580\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m signaled:\n\u001b[0;32m--> 581\u001b[0m     signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cond\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m signaled\n",
      "File \u001b[0;32m/opt/miniconda3/envs/office/lib/python3.9/threading.py:316\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    314\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    315\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 316\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    317\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    318\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m waiter\u001b[38;5;241m.\u001b[39macquire(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%pip install deepface\n",
    "%pip install tensorflow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "office",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
